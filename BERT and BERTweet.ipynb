{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TLaRd3-WGL6L"},"outputs":[],"source":["# Install deps\n","# !python -m spacy download en_core_web_md\n","# !pip install torch torchvision\n","# !pip install transformers\n","# !pip3 install nltk emoji==0.6.0\n","# !pip install optuna"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":187,"status":"ok","timestamp":1680292677920,"user":{"displayName":"Daniela De los Santos","userId":"01341847845262557810"},"user_tz":-120},"id":"Mrp7BUmvGL6N"},"outputs":[],"source":["#dataset\n","import pandas as pd\n","from datasets import Dataset \n","# utils\n","from sklearn.utils import shuffle\n","from sklearn.metrics import classification_report\n","#visualize\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# modeling\n","import spacy\n","from transformers import AutoTokenizer, TFAutoModel\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.optimizers import Adam\n","import tensorflow as tf\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import recall_score, precision_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","import numpy as np\n","\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import Trainer, TrainingArguments\n","from datasets import load_metric\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0VFc0z4jGL6O"},"source":["Reader functions:"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1680290163223,"user":{"displayName":"Daniela De los Santos","userId":"01341847845262557810"},"user_tz":-120},"id":"VI2ibpP9GL6P"},"outputs":[],"source":["# We define a function to read our already pre-processed data\n","def reader_df(topic):\n","  path_train = \"cleaned_df/stance_\" + topic + \"_train_cleaned.csv\"\n","  path_test = \"cleaned_df/stance_\" + topic + \"_test_cleaned.csv\"\n","  path_val = \"cleaned_df/stance_\" + topic + \"_validation_cleaned.csv\"\n","  df_train = pd.read_csv(path_train)\n","  df_val = pd.read_csv(path_val)\n","  df_test = pd.read_csv(path_test)\n","\n","  X_train = df_train.loc[:, 'text'].values\n","  y_train = df_train.loc[:, 'label'].values\n","\n","  X_test = df_test.loc[:, 'text'].values\n","  y_test = df_test.loc[:, 'label'].values\n","\n","  X_val = df_val.loc[:, 'text'].values\n","  y_val = df_val.loc[:, 'label'].values\n","\n","  return X_train, X_test, y_train, y_test, X_val, y_val"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1680290163859,"user":{"displayName":"Daniela De los Santos","userId":"01341847845262557810"},"user_tz":-120},"id":"k1kf6iVbGL6Q"},"outputs":[],"source":["# During finetuning, we might want to use the specific tokenizer functions of BERT models, so we define a function to read the raw data.\n","def raw_reader_and_tokenize(topic):\n","    path_X_train = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/stance/\" + topic + \"/train_text.txt\"\n","    path_X_test = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/stance/\" + topic + \"/test_text.txt\"\n","    path_X_val = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/stance/\" + topic + \"/val_text.txt\"\n","    path_y_train = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/stance/\" + topic + \"/train_labels.txt\"\n","    path_y_test = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/stance/\" + topic + \"/test_labels.txt\"\n","    path_y_val = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/stance/\" + topic + \"/val_labels.txt\"\n","\n","    X_train = pd.read_table(path_X_train, header=None)\n","    X_test = pd.read_table(path_X_test, header=None)\n","    X_val = pd.read_table(path_X_val, header=None)\n","    y_train = pd.read_table(path_y_train, header=None)\n","    y_test = pd.read_table(path_y_test, header=None)\n","    y_val = pd.read_table(path_y_val, header=None)\n","\n","    X_train = X_train.rename(columns={0: \"text\"})\n","    X_test = X_test.rename(columns={0: \"text\"})\n","    y_train = y_train.rename(columns={0: \"label\"})\n","    y_test = y_test.rename(columns={0: \"label\"})\n","    X_val = X_val.rename(columns={0: \"text\"})\n","    y_val = y_val.rename(columns={0: \"label\"})\n","\n","    # Convert the DataFrame to a Hugging Face Dataset\n","    train_dataset = Dataset.from_dict({\"text\": X_train[\"text\"].values, \"label\": y_train[\"label\"].values})\n","    val_dataset = Dataset.from_dict({\"text\": X_val[\"text\"].values, \"label\": y_val[\"label\"].values})\n","    test_dataset = Dataset.from_dict({\"text\": X_test[\"text\"].values, \"label\": y_test[\"label\"].values})\n","\n","    # Tokenize the training and validation datasets\n","    train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n","    val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n","    test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n","\n","    # Set the format of the datasets to PyTorch tensors\n","    train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","    val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","    test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","\n","\n","    return train_dataset, val_dataset, test_dataset\n","    \n","    "]},{"cell_type":"markdown","metadata":{"id":"CaHhPR7xGL6R"},"source":["+ **DistilBERT** model\n","\n","As a first test, we can calculate how accurate the predictions are using a DistilBert model. The code below uses `DistilBertTokenizerFast` on our already pre-processed datasets. \n","We apply the `DistilBertForSequenceClassification` model, which is the most appropiate for our current task of multi-class classification.\n","\n","We will implement this first model in the `stances-feminist` dataset, just to test it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IASvITwXGL6R"},"outputs":[],"source":["from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n","from sklearn.metrics import f1_score, precision_score, accuracy_score\n","import torch\n","\n","X_train, X_test, y_train, y_test, X_val, y_val = reader_df(\"feminist\")\n","\n","\n","\n","# Load the tokenizer and model\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n","\n","# Prepare the data\n","train_texts = X_train.tolist() \n","train_labels = y_train.tolist()\n","test_texts = X_test.tolist()\n","test_labels = y_test.tolist()\n","\n","# Tokenize the data\n","train_encodings = tokenizer(train_texts, return_tensors='pt', padding=True, truncation=True)\n","test_encodings = tokenizer(test_texts, return_tensors='pt', padding=True, truncation=True)\n","\n","# Convert to torch Dataset\n","class TweetDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = TweetDataset(train_encodings, train_labels)\n","test_dataset = TweetDataset(test_encodings, test_labels)\n","\n","# Train the model\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # useful to improve computing time, depending on our pc characteristics\n","model.to(device)\n","model.train()"]},{"cell_type":"markdown","metadata":{"id":"NpzGNxKrGL6S"},"source":["In the code below we use the PyTorch library to create a DataLoader object for the train_dataset with a batch size of 16. This means that the data in train_dataset will be divided into batches of 16 samples each and fed into the model for training.\n","We also create an Adam optimizer object with a learning rate of 5e-5. The optimizer is used to update the modelâ€™s parameters during training to minimize the loss function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXsGQwf1GL6S","outputId":"4541971d-b782-4a31-dc46-0fcb44e401e5"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","C:\\Users\\danid\\AppData\\Local\\Temp\\ipykernel_18520\\2094193682.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"]},{"name":"stdout","output_type":"stream","text":["F1 score: 0.5426585516619846\n","Precision: 0.6004088504088504\n","Accuracy: 0.5719298245614035\n"]}],"source":["\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n","optim = torch.optim.Adam(model.parameters(), lr=5e-5)\n","\n","for epoch in range(3):\n","    for batch in train_loader:\n","        optim.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs[0]\n","        loss.backward()\n","        optim.step()\n","\n","# Evaluate the model on the test set\n","model.eval()\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16)\n","predictions = []\n","true_labels = []\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs[0]\n","        predictions.extend(logits.argmax(dim=-1).cpu().numpy())\n","        true_labels.extend(labels.cpu().numpy())\n","\n","# Calculate metrics\n","f1 = f1_score(true_labels, predictions, average='macro')\n","precision = precision_score(true_labels, predictions, average='macro')\n","accuracy = accuracy_score(true_labels, predictions)\n","\n","print(f'F1 score: {f1}')\n","print(f'Precision: {precision}')\n","print(f'Accuracy: {accuracy}')"]},{"cell_type":"markdown","metadata":{"id":"EoFGRp5cGL6T"},"source":["We have an F1 of 54 with this implementation. How can we improve this?"]},{"cell_type":"markdown","metadata":{"id":"zCpCuOk5GL6T"},"source":["#### Fine-tuned BERTweet implementation"]},{"cell_type":"markdown","metadata":{"id":"cBv2yxrtGL6U"},"source":["BERT is a pre-trained language model that has been shown to achieve state-of-the-art performance on a wide range of natural language processing tasks. However, BERT was pre-trained on a large corpus of general text, which may not be representative of the language used in tweets.\n","\n","Tweets are known to have unique characteristics that can make them more challenging to classify compared to other types of text. For example, tweets are often shorter, contain a lot of noise (such as typos and slang), and can have complex grammatical structures that are not found in more formal writing.\n","\n","Additionally, the use of hashtags, emojis, and other special characters in tweets can make it difficult for BERT to understand the context and sentiment of the tweet. Pre-processing and cleaning the tweets can help to mitigate some of these issues, but there is still a limit to the effectiveness of this approach.\n","\n","To address these challenges, researchers have developed specialized versions of BERT for use with social media data. For example, BERTweet is a variant of BERT that has been trained specifically on tweets and has been shown to outperform generic BERT on tweet classification tasks."]},{"cell_type":"markdown","metadata":{"id":"zZfPPkBfGL6U"},"source":["+ **BERTweet**"]},{"cell_type":"markdown","metadata":{"id":"Q7Qb1s2MGL6V"},"source":["First we define a `compute_metrics` function that calls precision, recall and F1, using a macro average."]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":2098,"status":"ok","timestamp":1680290984115,"user":{"displayName":"Daniela De los Santos","userId":"01341847845262557810"},"user_tz":-120},"id":"5XSTZ4JcGL6V"},"outputs":[],"source":["precision = load_metric(\"precision\")\n","recall = load_metric(\"recall\")\n","f1 = load_metric(\"f1\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return {\n","        \"precision\": precision.compute(predictions=predictions, references=labels, average=\"macro\"),\n","        \"recall\": recall.compute(predictions=predictions, references=labels, average=\"macro\"),\n","        \"f1\": f1.compute(predictions=predictions, references=labels, average=\"macro\"),\n","    }"]},{"cell_type":"markdown","metadata":{"id":"JHNym4HdGL6V"},"source":["We define the tokenizer from Bertweet, and implement the five models after that."]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":279,"status":"ok","timestamp":1680290984391,"user":{"displayName":"Daniela De los Santos","userId":"01341847845262557810"},"user_tz":-120},"id":"FW_ZcjFuGL6W","outputId":"c1134630-7656-49bc-c92b-db200682e4a7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# Load the pre-trained BERTweet tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)\n","\n","# Define a function to tokenize the dataset\n","def tokenize(batch):\n","    return tokenizer(batch[\"text\"], padding=True, truncation=True)"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4447,"status":"ok","timestamp":1680290988836,"user":{"displayName":"Daniela De los Santos","userId":"01341847845262557810"},"user_tz":-120},"id":"VJsRQw1IGY-o","outputId":"271488b4-e54c-4f46-b6d6-cf06b0b1b2e1"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load the pre-trained BERTweet model for sequence classification\n","model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=3)"]},{"cell_type":"markdown","metadata":{"id":"rTgtPh8wGL6X"},"source":["### Stance: climate"]},{"cell_type":"markdown","metadata":{"id":"lF3fBeXWGL6X"},"source":["In this case, instead of our pre-processed datasets, we will import the raw data.\n","(As a note, we tried an implementation using our pre-processed datasets, but the scores were slightly worse in all cases. For this alternative implementation, see code that is commented out below.)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["2a5237ef3fe048c98100e83e566d8d18","1e5b34f86776453a91d538fe6a2652ce","c109a0c08f524133afd9d303ad186508"]},"id":"-uIoIF05GL6X","outputId":"7a949bd2-c295-41a4-e3d0-6c44b75edfab"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a5237ef3fe048c98100e83e566d8d18","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/355 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e5b34f86776453a91d538fe6a2652ce","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/40 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c109a0c08f524133afd9d303ad186508","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/169 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_dataset, val_dataset, test_dataset = raw_reader_and_tokenize(\"climate\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fTSzBuVGL6X"},"outputs":[],"source":["### ALTERNATIVE IMPLEMENTATION WITH OUR PRE-PROCESSED DATA, INSTEAD OF RAW DATA\n","\n","# Convert the DataFrame to a Hugging Face Dataset\n","# X_train, X_test, y_train, y_test, X_val, y_val = reader_df(\"climate\")\n","\n","# train_dataset = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n","# val_dataset = Dataset.from_dict({\"text\": X_val, \"label\": y_val})\n","# test_dataset = Dataset.from_dict({\"text\": X_test, \"label\": y_test})\n","\n","# # Tokenize the training and validation datasets\n","# train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n","# val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n","\n","# # Set the format of the datasets to PyTorch tensors\n","# train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","# val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["abeb6cde52514c9db7b84eefd65bee7b","9a4f51854cf14fb6b6dfc9d8544b8ea3","3706c883ffdf48d585e9bd00ff814d3a","0989051afe81401fa32089bff6d7a608","1708909cabb44e5a85733c17bf75ddac","4deec72421a944b08aad12c33ecb57d4"]},"id":"6wuUSIrzGL6Y","outputId":"4f5c0234-a4d0-4626-e4a0-dd140658bd79"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at C:\\Users\\danid/.cache\\huggingface\\hub\\models--vinai--bertweet-base\\snapshots\\118ab1d567653bec16bbb081eafb6f8942f72108\\config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/bertweet-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"transformers_version\": \"4.24.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at C:\\Users\\danid/.cache\\huggingface\\hub\\models--vinai--bertweet-base\\snapshots\\118ab1d567653bec16bbb081eafb6f8942f72108\\pytorch_model.bin\n","Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 355\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 60\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"abeb6cde52514c9db7b84eefd65bee7b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/60 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a4f51854cf14fb6b6dfc9d8544b8ea3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.5839727195225916}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6115779645191409}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.5971479500891265}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-12\n","Configuration saved in ./results\\checkpoint-12\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.5253016352653503, 'eval_precision': {'precision': 0.5839727195225916}, 'eval_recall': {'recall': 0.6115779645191409}, 'eval_f1': {'f1': 0.5971479500891265}, 'eval_runtime': 1.7267, 'eval_samples_per_second': 23.166, 'eval_steps_per_second': 1.158, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-12\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3706c883ffdf48d585e9bd00ff814d3a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.5444096133751306}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.5294117647058824}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.518095238095238}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-24\n","Configuration saved in ./results\\checkpoint-24\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.6195958852767944, 'eval_precision': {'precision': 0.5444096133751306}, 'eval_recall': {'recall': 0.5294117647058824}, 'eval_f1': {'f1': 0.518095238095238}, 'eval_runtime': 1.9658, 'eval_samples_per_second': 20.348, 'eval_steps_per_second': 1.017, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-24\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0989051afe81401fa32089bff6d7a608","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-36\n","Configuration saved in ./results\\checkpoint-36\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.4208868443965912, 'eval_precision': {'precision': 0.6041666666666666}, 'eval_recall': {'recall': 0.6274509803921569}, 'eval_f1': {'f1': 0.6141414141414141}, 'eval_runtime': 1.6987, 'eval_samples_per_second': 23.548, 'eval_steps_per_second': 1.177, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-36\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1708909cabb44e5a85733c17bf75ddac","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-48\n","Configuration saved in ./results\\checkpoint-48\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.3590737581253052, 'eval_precision': {'precision': 0.6041666666666666}, 'eval_recall': {'recall': 0.6274509803921569}, 'eval_f1': {'f1': 0.6141414141414141}, 'eval_runtime': 1.6977, 'eval_samples_per_second': 23.561, 'eval_steps_per_second': 1.178, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-48\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4deec72421a944b08aad12c33ecb57d4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-60\n","Configuration saved in ./results\\checkpoint-60\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.37791839241981506, 'eval_precision': {'precision': 0.6041666666666666}, 'eval_recall': {'recall': 0.6274509803921569}, 'eval_f1': {'f1': 0.6141414141414141}, 'eval_runtime': 1.7367, 'eval_samples_per_second': 23.032, 'eval_steps_per_second': 1.152, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-60\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-48 (score: 0.3590737581253052).\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 333.8925, 'train_samples_per_second': 5.316, 'train_steps_per_second': 0.18, 'train_loss': 0.48289686838785806, 'epoch': 5.0}\n"]},{"data":{"text/plain":["TrainOutput(global_step=60, training_loss=0.48289686838785806, metrics={'train_runtime': 333.8925, 'train_samples_per_second': 5.316, 'train_steps_per_second': 0.18, 'train_loss': 0.48289686838785806, 'epoch': 5.0})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Define the training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    evaluation_strategy='epoch',\n","    save_strategy='epoch',\n","    learning_rate=1e-4,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=5,\n","    logging_dir='./logs',\n","    load_best_model_at_end=True,\n","    seed=1\n",")\n","\n","\n","# Create a Trainer instance\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Fine-tune the model on the training dataset\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["31bbc5d3f9ba4941b6b1d1b0b2406138"]},"id":"_LIIdfeYGL6Y","outputId":"f6ea5ed0-e1a9-47ea-a183-863359c9d53b"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 169\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31bbc5d3f9ba4941b6b1d1b0b2406138","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.5184569952011812}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.5770034843205575}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.5450520028833282}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"]}],"source":["# Evaluate the trained model in the test set\n","\n","# trainer.compute_metrics=compute_metrics\n","# (if we want to use different metrics (e.g. weighted average), just update the compute_metrics function\n","#  and run the line above)\n","results = trainer.evaluate(test_dataset)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3NnqG9soGL6Y","outputId":"ee9e961f-b4a2-4def-876e-e2cfbe8fc40e"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'precision': 0.5184569952011812}\n","{'recall': 0.5770034843205575}\n","{'f1': 0.5450520028833282}\n"]}],"source":["print(results[\"eval_precision\"])\n","print(results[\"eval_recall\"])\n","print(results[\"eval_f1\"])"]},{"cell_type":"markdown","metadata":{"id":"fzfM3tsOGL6Y"},"source":["To assess possible biases, we can check how well we are predicting each class:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["871955b76468400f922b05d3c934ebd8"]},"id":"AYTaRN0tGL6Y","outputId":"3bc86a60-cf09-4a5a-ba79-effbb927f41b"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 169\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"871955b76468400f922b05d3c934ebd8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Class: 0\n","Precision: 0.6744186046511628\n","Recall: 0.8285714285714286\n","F1-score: 0.7435897435897435\n","\n","Class: 1\n","Precision: 0.0\n","Recall: 0.0\n","F1-score: 0.0\n","\n","Class: 2\n","Precision: 0.8809523809523809\n","Recall: 0.9024390243902439\n","F1-score: 0.8915662650602411\n","\n","Class: macro avg\n","Precision: 0.5184569952011812\n","Recall: 0.5770034843205575\n","F1-score: 0.5450520028833282\n","\n","Class: weighted avg\n","Precision: 0.7808390178694293\n","Recall: 0.8284023668639053\n","F1-score: 0.8028892995742644\n","\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["# Generate predictions for the test set\n","test_predictions = trainer.predict(test_dataset).predictions\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","\n","# Compute the classification report\n","report = classification_report(test_dataset['label'], test_predictions, output_dict=True)\n","\n","# Print the classification report\n","for label in report:\n","    if label != 'accuracy':\n","        print(f\"Class: {label}\")\n","        print(f\"Precision: {report[label]['precision']}\")\n","        print(f\"Recall: {report[label]['recall']}\")\n","        print(f\"F1-score: {report[label]['f1-score']}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"X900-N_AGL6Z"},"source":["To compare with the original benchmarking methodology, we take the average of F1 scores for against and favor clases:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gUchnkWcGL6a","outputId":"d42f95d8-c090-4be5-8ecd-61adb476c061"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final F1 - Stance CLIMATE: 0.44578313253012053\n"]}],"source":["# Final evaluation\n","f1_against = report['1']['f1-score']\n","f1_favor = report['2']['f1-score']\n","tweeteval_result = (f1_against+f1_favor) / 2\n","print(\"Final F1 - Stance CLIMATE: \" + str(tweeteval_result))"]},{"cell_type":"markdown","metadata":{"id":"JEI4XfZPGL6a"},"source":["###  Stance: atheism"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["e3c79fecd9834ad1b0f7664d3dc54773","d120772e5e8c4816a98c903d4a1c562d","1196723d1fea4ee18a26cb677931bc1d","057cc1c0f42c41a8976d5949bcb84ad0","3f7aa89c4aae4e439378217bff9de594","3845b9dfa5ab4ae09e5d2ce0e73a72f7","0ada16094ff24447b44affdedfe37ea9","8a05e5ff54bf43a6ab9bc9746df5e823","a2358461d440429ba54cf5df503ed522","d2e638bbf3af486e8bb0a206f5451064"]},"id":"9AfBFbU8GL6a","outputId":"d65f9769-3bfd-4138-e745-c7c1f65c0f44"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3c79fecd9834ad1b0f7664d3dc54773","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/461 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d120772e5e8c4816a98c903d4a1c562d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/52 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1196723d1fea4ee18a26cb677931bc1d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/220 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 461\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 75\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"057cc1c0f42c41a8976d5949bcb84ad0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/75 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 52\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f7aa89c4aae4e439378217bff9de594","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.4794326241134752}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.4336917562724014}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.41327300150829566}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-15\n","Configuration saved in ./results\\checkpoint-15\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.957466185092926, 'eval_precision': {'precision': 0.4794326241134752}, 'eval_recall': {'recall': 0.4336917562724014}, 'eval_f1': {'f1': 0.41327300150829566}, 'eval_runtime': 2.4984, 'eval_samples_per_second': 20.814, 'eval_steps_per_second': 0.801, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-15\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 52\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3845b9dfa5ab4ae09e5d2ce0e73a72f7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.646011396011396}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.5433094384707288}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.5493386243386243}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-30\n","Configuration saved in ./results\\checkpoint-30\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.744162917137146, 'eval_precision': {'precision': 0.646011396011396}, 'eval_recall': {'recall': 0.5433094384707288}, 'eval_f1': {'f1': 0.5493386243386243}, 'eval_runtime': 2.4655, 'eval_samples_per_second': 21.091, 'eval_steps_per_second': 0.811, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-30\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 52\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ada16094ff24447b44affdedfe37ea9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.6708333333333334}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6684587813620072}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6674767727399306}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-45\n","Configuration saved in ./results\\checkpoint-45\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.6782239079475403, 'eval_precision': {'precision': 0.6708333333333334}, 'eval_recall': {'recall': 0.6684587813620072}, 'eval_f1': {'f1': 0.6674767727399306}, 'eval_runtime': 2.4587, 'eval_samples_per_second': 21.149, 'eval_steps_per_second': 0.813, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-45\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 52\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a05e5ff54bf43a6ab9bc9746df5e823","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.7523809523809524}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6899641577060932}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.7038809144072302}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-60\n","Configuration saved in ./results\\checkpoint-60\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.7633729577064514, 'eval_precision': {'precision': 0.7523809523809524}, 'eval_recall': {'recall': 0.6899641577060932}, 'eval_f1': {'f1': 0.7038809144072302}, 'eval_runtime': 2.4514, 'eval_samples_per_second': 21.213, 'eval_steps_per_second': 0.816, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-60\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 52\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2358461d440429ba54cf5df503ed522","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.7293650793650794}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.7425328554360813}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.7160370634354954}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-75\n","Configuration saved in ./results\\checkpoint-75\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.758226752281189, 'eval_precision': {'precision': 0.7293650793650794}, 'eval_recall': {'recall': 0.7425328554360813}, 'eval_f1': {'f1': 0.7160370634354954}, 'eval_runtime': 2.3626, 'eval_samples_per_second': 22.009, 'eval_steps_per_second': 0.847, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-75\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-45 (score: 0.6782239079475403).\n","The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 220\n","  Batch size = 32\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 411.1427, 'train_samples_per_second': 5.606, 'train_steps_per_second': 0.182, 'train_loss': 0.5718584187825521, 'epoch': 5.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2e638bbf3af486e8bb0a206f5451064","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/7 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Class: 0\n","Precision: 0.5348837209302325\n","Recall: 0.8214285714285714\n","F1-score: 0.647887323943662\n","\n","Class: 1\n","Precision: 0.9\n","Recall: 0.7875\n","F1-score: 0.84\n","\n","Class: 2\n","Precision: 0.5405405405405406\n","Recall: 0.625\n","F1-score: 0.5797101449275363\n","\n","Class: macro avg\n","Precision: 0.658474753823591\n","Recall: 0.744642857142857\n","F1-score: 0.6891991562903993\n","\n","Class: weighted avg\n","Precision: 0.8012456431061082\n","Recall: 0.7681818181818182\n","F1-score: 0.7776889532186532\n","\n"]}],"source":["# Import dataset\n","train_dataset, val_dataset, test_dataset = raw_reader_and_tokenize(\"atheism\")\n","\n","\n","# Create a Trainer instance\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Fine-tune the model on the training dataset\n","trainer.train()\n","\n","# Generate predictions for the test set\n","test_predictions = trainer.predict(test_dataset).predictions\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","\n","# Compute the classification report\n","report = classification_report(test_dataset['label'], test_predictions, output_dict=True)\n","\n","# Print the classification report\n","for label in report:\n","    if label != 'accuracy':\n","        print(f\"Class: {label}\")\n","        print(f\"Precision: {report[label]['precision']}\")\n","        print(f\"Recall: {report[label]['recall']}\")\n","        print(f\"F1-score: {report[label]['f1-score']}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLa-kmpGGL6a","outputId":"364b1a9d-554f-4149-dfe4-0721ca5d1d2c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final F1 - Stance ATHEISM: 0.7098550724637681\n"]}],"source":["# Final evaluation\n","f1_against = report['1']['f1-score']\n","f1_favor = report['2']['f1-score']\n","tweeteval_result_atheism = (f1_against+f1_favor) / 2\n","print(\"Final F1 - Stance ATHEISM: \" + str(tweeteval_result_atheism))"]},{"cell_type":"markdown","metadata":{"id":"g4iPYcjZGL6a"},"source":["### Stance: feminist"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["e5d5dd47e0f4446b9fef6d5e37a549b9","9d837f6b1e704ad5bba2e702446eee09","8f4ed61e00ae44648521ee57724690fb","7ac7e64b4c7c460fa27ff476b61168e0","486c412653974130bd46cf8ea2aed9ae","12b1a388311f4b919ca58817a3f04965","83df49946cab451392da862b1912c383","2f7f9f4c70e14daaa913390e64695db0","b2cb3f04459f4ff9be910129ae892aa4","fe0f7d14bbd44287b40e3805095f78fc"]},"id":"2w-5_s7cGL6a","outputId":"69d9dcd8-565b-4e06-e552-a0b3dad1d5f8"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e5d5dd47e0f4446b9fef6d5e37a549b9","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/597 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d837f6b1e704ad5bba2e702446eee09","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/67 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f4ed61e00ae44648521ee57724690fb","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/285 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 597\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 95\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ac7e64b4c7c460fa27ff476b61168e0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/95 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 67\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"486c412653974130bd46cf8ea2aed9ae","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.6662768031189084}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.5153735153735154}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.47222222222222215}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-19\n","Configuration saved in ./results\\checkpoint-19\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8846456408500671, 'eval_precision': {'precision': 0.6662768031189084}, 'eval_recall': {'recall': 0.5153735153735154}, 'eval_f1': {'f1': 0.47222222222222215}, 'eval_runtime': 4.9132, 'eval_samples_per_second': 13.637, 'eval_steps_per_second': 0.611, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-19\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 67\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12b1a388311f4b919ca58817a3f04965","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.5913521176679072}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.558996558996559}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.5519460851128275}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-38\n","Configuration saved in ./results\\checkpoint-38\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.9805386662483215, 'eval_precision': {'precision': 0.5913521176679072}, 'eval_recall': {'recall': 0.558996558996559}, 'eval_f1': {'f1': 0.5519460851128275}, 'eval_runtime': 4.5639, 'eval_samples_per_second': 14.68, 'eval_steps_per_second': 0.657, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-38\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 67\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83df49946cab451392da862b1912c383","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.5841491841491842}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.5275835275835276}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.5294920133629811}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-57\n","Configuration saved in ./results\\checkpoint-57\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.03778076171875, 'eval_precision': {'precision': 0.5841491841491842}, 'eval_recall': {'recall': 0.5275835275835276}, 'eval_f1': {'f1': 0.5294920133629811}, 'eval_runtime': 4.4445, 'eval_samples_per_second': 15.075, 'eval_steps_per_second': 0.675, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-57\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 67\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f7f9f4c70e14daaa913390e64695db0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.5459595959595959}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.5308025308025308}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.5369093908330388}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-76\n","Configuration saved in ./results\\checkpoint-76\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.9985182881355286, 'eval_precision': {'precision': 0.5459595959595959}, 'eval_recall': {'recall': 0.5308025308025308}, 'eval_f1': {'f1': 0.5369093908330388}, 'eval_runtime': 4.3904, 'eval_samples_per_second': 15.26, 'eval_steps_per_second': 0.683, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-76\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 67\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2cb3f04459f4ff9be910129ae892aa4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.5438228438228437}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.5481185481185481}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.5421310368118878}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-95\n","Configuration saved in ./results\\checkpoint-95\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.0821036100387573, 'eval_precision': {'precision': 0.5438228438228437}, 'eval_recall': {'recall': 0.5481185481185481}, 'eval_f1': {'f1': 0.5421310368118878}, 'eval_runtime': 4.415, 'eval_samples_per_second': 15.176, 'eval_steps_per_second': 0.68, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-95\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-19 (score: 0.8846456408500671).\n","The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 285\n","  Batch size = 32\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 735.8505, 'train_samples_per_second': 4.057, 'train_steps_per_second': 0.129, 'train_loss': 0.6242399115311472, 'epoch': 5.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe0f7d14bbd44287b40e3805095f78fc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Class: 0\n","Precision: 0.49206349206349204\n","Recall: 0.7045454545454546\n","F1-score: 0.5794392523364486\n","\n","Class: 1\n","Precision: 0.7156398104265402\n","Recall: 0.825136612021858\n","F1-score: 0.766497461928934\n","\n","Class: 2\n","Precision: 0.36363636363636365\n","Recall: 0.06896551724137931\n","F1-score: 0.11594202898550723\n","\n","Class: macro avg\n","Precision: 0.5237798887087987\n","Recall: 0.5328825279362306\n","F1-score: 0.48729291441696326\n","\n","Class: weighted avg\n","Precision: 0.6094869756131915\n","Recall: 0.6526315789473685\n","F1-score: 0.6052245625156423\n","\n"]}],"source":["# Import dataset\n","train_dataset, val_dataset, test_dataset = raw_reader_and_tokenize(\"feminist\")\n","\n","\n","# Create a Trainer instance\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Fine-tune the model on the training dataset\n","trainer.train()\n","\n","# Generate predictions for the test set\n","test_predictions = trainer.predict(test_dataset).predictions\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","\n","# Compute the classification report\n","report = classification_report(test_dataset['label'], test_predictions, output_dict=True)\n","\n","# Print the classification report\n","for label in report:\n","    if label != 'accuracy':\n","        print(f\"Class: {label}\")\n","        print(f\"Precision: {report[label]['precision']}\")\n","        print(f\"Recall: {report[label]['recall']}\")\n","        print(f\"F1-score: {report[label]['f1-score']}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DdmieIZGL6a","outputId":"976bbdb5-0ef0-4c66-f618-bc7575d67db5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final F1 - Stance FEMINIST: 0.4412197454572206\n"]}],"source":["# Final evaluation\n","f1_against = report['1']['f1-score']\n","f1_favor = report['2']['f1-score']\n","tweeteval_result_feminist = (f1_against+f1_favor) / 2\n","print(\"Final F1 - Stance FEMINIST: \" + str(tweeteval_result_feminist))"]},{"cell_type":"markdown","metadata":{"id":"OxFCAON7GL6b"},"source":["### Stance: Abortion"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["d84e730cdf3f42b2b6d196d1491a16dc","e25530f203644912a3443714e3f9f98c","6788a285356b45a19b07876329286055","83f4702eeec04cdfa88ccaefbab70dfb","c2eb4156831147ee9ede4fdb4bbaaa77","0358f0ee87a24abd9ef592d6ecfd9e58","1393f8e4151c466f8987a641ff25cec5","bbebf0ec1186479c8100f19f54089a93","96f943b9b63641c583f4195012fe73fd","c071e9903a3c443b9f38ba1c1fe83e63"]},"id":"9R075WakGL6c","outputId":"a012a49d-b203-4b2b-9503-b0ce84e524d6"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d84e730cdf3f42b2b6d196d1491a16dc","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/587 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e25530f203644912a3443714e3f9f98c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/66 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6788a285356b45a19b07876329286055","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/280 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 587\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 95\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83f4702eeec04cdfa88ccaefbab70dfb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/95 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 66\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2eb4156831147ee9ede4fdb4bbaaa77","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.7359788359788361}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6851851851851851}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.7034077034077034}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-19\n","Configuration saved in ./results\\checkpoint-19\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.6615263223648071, 'eval_precision': {'precision': 0.7359788359788361}, 'eval_recall': {'recall': 0.6851851851851851}, 'eval_f1': {'f1': 0.7034077034077034}, 'eval_runtime': 2.8533, 'eval_samples_per_second': 23.131, 'eval_steps_per_second': 1.051, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-19\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 66\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0358f0ee87a24abd9ef592d6ecfd9e58","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.7171877760113055}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.7129629629629629}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.714851054577082}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-38\n","Configuration saved in ./results\\checkpoint-38\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.6878447532653809, 'eval_precision': {'precision': 0.7171877760113055}, 'eval_recall': {'recall': 0.7129629629629629}, 'eval_f1': {'f1': 0.714851054577082}, 'eval_runtime': 3.2953, 'eval_samples_per_second': 20.029, 'eval_steps_per_second': 0.91, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-38\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 66\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1393f8e4151c466f8987a641ff25cec5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.6828282828282828}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.7037037037037037}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6902844873859366}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-57\n","Configuration saved in ./results\\checkpoint-57\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.6775017976760864, 'eval_precision': {'precision': 0.6828282828282828}, 'eval_recall': {'recall': 0.7037037037037037}, 'eval_f1': {'f1': 0.6902844873859366}, 'eval_runtime': 2.7448, 'eval_samples_per_second': 24.045, 'eval_steps_per_second': 1.093, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-57\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 66\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bbebf0ec1186479c8100f19f54089a93","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.6633986928104575}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6759259259259259}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6683760683760683}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-76\n","Configuration saved in ./results\\checkpoint-76\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.6935228109359741, 'eval_precision': {'precision': 0.6633986928104575}, 'eval_recall': {'recall': 0.6759259259259259}, 'eval_f1': {'f1': 0.6683760683760683}, 'eval_runtime': 3.3371, 'eval_samples_per_second': 19.778, 'eval_steps_per_second': 0.899, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-76\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 66\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96f943b9b63641c583f4195012fe73fd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.7282765737874097}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.7777777777777777}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.7378578410836475}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-95\n","Configuration saved in ./results\\checkpoint-95\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.7481910586357117, 'eval_precision': {'precision': 0.7282765737874097}, 'eval_recall': {'recall': 0.7777777777777777}, 'eval_f1': {'f1': 0.7378578410836475}, 'eval_runtime': 3.1779, 'eval_samples_per_second': 20.768, 'eval_steps_per_second': 0.944, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-95\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-19 (score: 0.6615263223648071).\n","The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 280\n","  Batch size = 32\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 899.8938, 'train_samples_per_second': 3.261, 'train_steps_per_second': 0.106, 'train_loss': 0.4689614145379317, 'epoch': 5.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c071e9903a3c443b9f38ba1c1fe83e63","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Class: 0\n","Precision: 0.45454545454545453\n","Recall: 0.7777777777777778\n","F1-score: 0.5737704918032788\n","\n","Class: 1\n","Precision: 0.7722222222222223\n","Recall: 0.7354497354497355\n","F1-score: 0.7533875338753387\n","\n","Class: 2\n","Precision: 0.4782608695652174\n","Recall: 0.2391304347826087\n","F1-score: 0.3188405797101449\n","\n","Class: macro avg\n","Precision: 0.5683428487776313\n","Recall: 0.584119316003374\n","F1-score: 0.5486662017962541\n","\n","Class: weighted avg\n","Precision: 0.6728733766233768\n","Recall: 0.6607142857142857\n","F1-score: 0.6531306525009043\n","\n"]}],"source":["# Import dataset\n","train_dataset, val_dataset, test_dataset = raw_reader_and_tokenize(\"abortion\")\n","\n","\n","# Create a Trainer instance\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Fine-tune the model on the training dataset\n","trainer.train()\n","\n","# Generate predictions for the test set\n","test_predictions = trainer.predict(test_dataset).predictions\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","\n","# Compute the classification report\n","report = classification_report(test_dataset['label'], test_predictions, output_dict=True)\n","\n","# Print the classification report\n","for label in report:\n","    if label != 'accuracy':\n","        print(f\"Class: {label}\")\n","        print(f\"Precision: {report[label]['precision']}\")\n","        print(f\"Recall: {report[label]['recall']}\")\n","        print(f\"F1-score: {report[label]['f1-score']}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4JJXhm2GL6c","outputId":"d4dff7c3-2b14-4a13-c6fa-1c6eef878984"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final F1 - Stance ABORTION: 0.5361140567927418\n"]}],"source":["# Final evaluation\n","f1_against = report['1']['f1-score']\n","f1_favor = report['2']['f1-score']\n","tweeteval_result_abortion = (f1_against+f1_favor) / 2\n","print(\"Final F1 - Stance ABORTION: \" + str(tweeteval_result_abortion))"]},{"cell_type":"markdown","metadata":{"id":"B6NAcUMXGL6c"},"source":["### Stance: Hillary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["07f5ed6c2d9548478ab14fe090681e30","bf5111f3e1f44e858eb87e7db90eec51","2695c31f0ef84e509fe8ff6abc9309ba","d5018c351a4448c5a464304b794acbe0","8fde833152424afbb6d32b1a5db9752b","068aba62311a4db69ec1e1133c7d7358","3085f51f072843949b23a76c495458ee","d1158a8a3958413083562b45c31f26a4","0a47a6cb2f41468baaff0f1d23d9cc0b","a102818554b74455b823a643b5a4fb29"]},"id":"Xb53m3rmGL6c","outputId":"6763badc-8536-473f-d364-b2931752d65c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07f5ed6c2d9548478ab14fe090681e30","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/587 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf5111f3e1f44e858eb87e7db90eec51","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/66 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2695c31f0ef84e509fe8ff6abc9309ba","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/280 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 587\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 95\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5018c351a4448c5a464304b794acbe0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/95 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 66\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8fde833152424afbb6d32b1a5db9752b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.6805555555555557}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.7037037037037037}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6562289562289562}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-19\n","Configuration saved in ./results\\checkpoint-19\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8476704955101013, 'eval_precision': {'precision': 0.6805555555555557}, 'eval_recall': {'recall': 0.7037037037037037}, 'eval_f1': {'f1': 0.6562289562289562}, 'eval_runtime': 2.7017, 'eval_samples_per_second': 24.43, 'eval_steps_per_second': 1.11, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-19\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 66\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"068aba62311a4db69ec1e1133c7d7358","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.7175925925925926}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.611111111111111}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6331569664902998}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-38\n","Configuration saved in ./results\\checkpoint-38\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8007002472877502, 'eval_precision': {'precision': 0.7175925925925926}, 'eval_recall': {'recall': 0.611111111111111}, 'eval_f1': {'f1': 0.6331569664902998}, 'eval_runtime': 2.9279, 'eval_samples_per_second': 22.542, 'eval_steps_per_second': 1.025, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-38\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 66\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3085f51f072843949b23a76c495458ee","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.7052287581699347}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.7222222222222223}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.7053092501368363}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-57\n","Configuration saved in ./results\\checkpoint-57\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.7921849489212036, 'eval_precision': {'precision': 0.7052287581699347}, 'eval_recall': {'recall': 0.7222222222222223}, 'eval_f1': {'f1': 0.7053092501368363}, 'eval_runtime': 2.7333, 'eval_samples_per_second': 24.147, 'eval_steps_per_second': 1.098, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-57\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 66\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1158a8a3958413083562b45c31f26a4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.7357142857142858}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.75}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.7392064807143514}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-76\n","Configuration saved in ./results\\checkpoint-76\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8476834297180176, 'eval_precision': {'precision': 0.7357142857142858}, 'eval_recall': {'recall': 0.75}, 'eval_f1': {'f1': 0.7392064807143514}, 'eval_runtime': 3.0218, 'eval_samples_per_second': 21.841, 'eval_steps_per_second': 0.993, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-76\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 66\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a47a6cb2f41468baaff0f1d23d9cc0b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.734640522875817}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.7592592592592594}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.7378215654077723}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-95\n","Configuration saved in ./results\\checkpoint-95\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8488245606422424, 'eval_precision': {'precision': 0.734640522875817}, 'eval_recall': {'recall': 0.7592592592592594}, 'eval_f1': {'f1': 0.7378215654077723}, 'eval_runtime': 3.5002, 'eval_samples_per_second': 18.856, 'eval_steps_per_second': 0.857, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-95\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-57 (score: 0.7921849489212036).\n","The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 280\n","  Batch size = 32\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 520.1074, 'train_samples_per_second': 5.643, 'train_steps_per_second': 0.183, 'train_loss': 0.3170971117521587, 'epoch': 5.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a102818554b74455b823a643b5a4fb29","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Class: 0\n","Precision: 0.5076923076923077\n","Recall: 0.7333333333333333\n","F1-score: 0.6\n","\n","Class: 1\n","Precision: 0.8544303797468354\n","Recall: 0.7142857142857143\n","F1-score: 0.7780979827089337\n","\n","Class: 2\n","Precision: 0.5263157894736842\n","Recall: 0.6521739130434783\n","F1-score: 0.5825242718446602\n","\n","Class: macro avg\n","Precision: 0.6294794923042758\n","Recall: 0.6999309868875087\n","F1-score: 0.6535407515178646\n","\n","Class: weighted avg\n","Precision: 0.7448000783360543\n","Recall: 0.7071428571428572\n","F1-score: 0.7173451258458674\n","\n"]}],"source":["# Import dataset\n","train_dataset, val_dataset, test_dataset = raw_reader_and_tokenize(\"abortion\")\n","\n","\n","# Create a Trainer instance\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Fine-tune the model on the training dataset\n","trainer.train()\n","\n","# Generate predictions for the test set\n","test_predictions = trainer.predict(test_dataset).predictions\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","\n","# Compute the classification report\n","report = classification_report(test_dataset['label'], test_predictions, output_dict=True)\n","\n","# Print the classification report\n","for label in report:\n","    if label != 'accuracy':\n","        print(f\"Class: {label}\")\n","        print(f\"Precision: {report[label]['precision']}\")\n","        print(f\"Recall: {report[label]['recall']}\")\n","        print(f\"F1-score: {report[label]['f1-score']}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgkumrQ3GL6d","outputId":"ae4975ea-3cba-4f8f-831b-d980e1c91d63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final F1 - Stance HILLARY: 0.680311127276797\n"]}],"source":["# Final evaluation\n","f1_against = report['1']['f1-score']\n","f1_favor = report['2']['f1-score']\n","tweeteval_result_hillary = (f1_against+f1_favor) / 2\n","print(\"Final F1 - Stance HILLARY: \" + str(tweeteval_result_hillary))"]},{"cell_type":"markdown","metadata":{"id":"GDLUfzYKGL6e"},"source":["The average F1 for all five datasets is:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pucEXhTGL6e","outputId":"3bbe9b25-a6b7-4e0b-ec4e-3d2a40060e03"},"outputs":[{"name":"stdout","output_type":"stream","text":["Global F1 mean: 0.5626566269041297\n","{'Climate': 0.44578313253012053, 'Abortion': 0.5361140567927418, 'Atheism': 0.7098550724637681, 'Feminist': 0.4412197454572206, 'Hillary': 0.680311127276797}\n"]}],"source":["f1_all = [tweeteval_result, tweeteval_result_abortion, tweeteval_result_atheism, tweeteval_result_feminist, tweeteval_result_hillary]\n","f1_all = np.array(f1_all)\n","f1_all_dic = {\"Climate\":tweeteval_result, \"Abortion\": tweeteval_result_abortion, \"Atheism\": tweeteval_result_atheism, \"Feminist\": tweeteval_result_feminist, \"Hillary\": tweeteval_result_hillary}\n","print(\"Global F1 mean: \" + str(np.mean(f1_all)))\n","print(f1_all_dic)"]},{"cell_type":"markdown","metadata":{"id":"Pbrb-3mJGL6e"},"source":["## How to improve the model: Hyperparameter tuning"]},{"cell_type":"markdown","metadata":{"id":"8DcxHDuwGL6e"},"source":["### Stance: Climate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["1936a365535d4703a72c3a22303b60ff","89cf893c33ba401582ab10eb1e7b93e1","53f725d895a7491ca8ee1486a577d757"]},"id":"pSKydR4oGL6e","outputId":"8a11a1d5-8fb7-4665-c6c4-054f07bce8f5"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1936a365535d4703a72c3a22303b60ff","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/355 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89cf893c33ba401582ab10eb1e7b93e1","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/40 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53f725d895a7491ca8ee1486a577d757","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/169 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_dataset, val_dataset, test_dataset = raw_reader_and_tokenize(\"climate\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["df14b6dc2d7945b3962a1221fbe3813e","46acac53d3914aa7bb9e839b6d025ed9","ed00adc6a35f4851a1a75b04cf33c7d2","c71cb8caade34a09ba0a6444fa67a02c","e484e0b0014f4ff5bf413b9863eee7a8","57233dfc782643ee93befc133b5b8a9c","fdd96abff8a243958ddb3af632ad67fe","7e4a75cd29094746bc6cd9bf1ace9948","54f35a24ea5042dbacc07f88df7d9f8e","9f1810b2934a4333a2a982df6463b1f9","88dcbccedfe1463ca3b0a54f15b575b0","f57551caf9db4ffeb9edf63a28d775de","ea8ae66ac5c94ad8826a39eee82b9bea","6cf78977f0d14a05ab92491532cfeab1","d94948e776bb438a9baedbc0b10177b4","a9633adffd8748758846babff790f50d","78ff24c395404d808ac067b9d926d63c","f11535e5ffe4493cbf9f225ce76c9bc6","da763e4763c940eca25cd343e816994c","6c174ae43a5c48238e6f71be3d723c2e","e6b72741ce174a74874fa6ce1a402ccf","97eaac2711cd4868af65b79c5952753a","d853b09d1c464ade99421799fe774192","0bd9a9f29e0247f286bd99aa6f88a758","efa55b750e8145dc8ae726a2f4405dea","226181f48e584ccda7b0a1db444e1baa","bf71843377244e8682aee1dd7cd29c8e","26baef544fe24287bfebc219d73dce89","77fa22a954374f29a1737870e75ead52","04c5eb75f54547b3bf6987ff09ee35e7","2dcee1b3c2134297b1d0d81359752239","c2f3b826192841d89f479f9ba0397deb","fd666ffb30f1420eaaee1a8a2b9b9ac8","247d4e49339e4e7c8faaeeb97a97e629","5881165a6c054df989547e5357b1f278","89636adbaa6a43cdb7cd2530c5a85179","62fb1b12455f4131b6ed583f3a41b325","8e56bac3e0bf4e5193f14b5b12c1ef65","3533b307d07a4a72be9d3fa9bfbb0a7d","4af0e098026f4c3e86458b29d5f1e87b","fa1c4bc6350d4a65859cde5277f2a300","74d10c8e24d9452b994a46e1e7fba453","34464a8a488b43eca722e8cee5a2f0b2","16ed97a5fa7648f19ea35fd2c30a1d01","e6172ee8262a4b988ddfc42d6031b7dd","ee3bd1654f5a44378c65a087b96f0a23","294bb56124f746328a95512e0f628dce","3a8957a50f3a4ab8bc47ce284015fc5e","a2448ed0eca14a789ae92439b0abeb19","3d8898037d0d442a93773ba1873abc5a","9d684e961a484c698a1cd366c587a360","bb6882237a8c4180b021a7a575e798a6","f1df6cc108d34a7ca09091d112a3315d","c6dfea5df5e747e09c684917bc4ad391","a6a6532adc6e4e6e8087aae6415071ad","9c9464e3e45944ba9a27860927ab1efe","023a98cf924c4878a9e8cce9fca28ce8","e1a3c1fbf9e4416288cd24527ce5d88c","97df338fae504d289c9dd9281e3aa2b6","e21167a7c48d45aababfb22e8b2421c0","2a632d32681d42b0bd7191195e820c70","7d928b432d5348d99ca3f18ad49dec9f","9d2ce4c337db49eeb05b4a060c704e18","ccac7c6370154d81acda10f5c7fce90c","1b4dc9afcd27476b85b63c78b1810d76","8cf8735c004c4b498004b817f804a9dc","da45ef3d0332425c86e209700e4bb180","03435635f3ed4007ace396d97af2637b","c3aa8819b14844bf86cd9110f2e0d11a","53f6a1d5703345bda2af55369f960255","997a131e64504f409c059f1fffee31a0","954b41d7ff73410c991efc438aede472","c0eb4091d82e40c8b57f76c8487165cc","1f3233e047214364bfd2b96e9e33a5c2","b1c34040a940446e9c21b62be53ffc48","fe465ddbfe49473e9ff3ba9e83525f6f","d126da5e80704f7e96c47148baa7b607","9169f0fa732f42929d538b6b5d6609bd","008298f076e64b448cd6e10df4810e47","cc5ab83691f34638a3b15415c0d4e8a0","18ea10b0ae534847bda285d26b8b121f","50155231419b4a6d9c811934c2a31557","d31891b9e1294332936c33d4658a6d55","4e2a0bf632ea4bc38b3de12b4b7c2732","ed710340fc894753be41df9ca8c11576","109a006e3e4d4c0d92984b786c7c59a0"]},"id":"igSMpzPDGL6e","outputId":"58675924-e518-4308-94c2-e4abf6977a40"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2023-03-31 19:18:27,205]\u001b[0m A new study created in memory with name: no-name-936b3d0b-3a1b-46aa-bc5f-066cbf0fa27f\u001b[0m\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 355\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 96\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df14b6dc2d7945b3962a1221fbe3813e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/96 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46acac53d3914aa7bb9e839b6d025ed9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'precision': 0.6023550724637681}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6115779645191409}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.606060606060606}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-12\n","Configuration saved in ./results\\checkpoint-12\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8130394220352173, 'eval_precision': {'precision': 0.6023550724637681}, 'eval_recall': {'recall': 0.6115779645191409}, 'eval_f1': {'f1': 0.606060606060606}, 'eval_runtime': 1.8872, 'eval_samples_per_second': 21.195, 'eval_steps_per_second': 1.59, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-12\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed00adc6a35f4851a1a75b04cf33c7d2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-24\n","Configuration saved in ./results\\checkpoint-24\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.7627484202384949, 'eval_precision': {'precision': 0.6041666666666666}, 'eval_recall': {'recall': 0.6274509803921569}, 'eval_f1': {'f1': 0.6141414141414141}, 'eval_runtime': 1.8085, 'eval_samples_per_second': 22.117, 'eval_steps_per_second': 1.659, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-24\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c71cb8caade34a09ba0a6444fa67a02c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-36\n","Configuration saved in ./results\\checkpoint-36\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.7815347909927368, 'eval_precision': {'precision': 0.6041666666666666}, 'eval_recall': {'recall': 0.6274509803921569}, 'eval_f1': {'f1': 0.6141414141414141}, 'eval_runtime': 1.7142, 'eval_samples_per_second': 23.334, 'eval_steps_per_second': 1.75, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-36\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e484e0b0014f4ff5bf413b9863eee7a8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-48\n","Configuration saved in ./results\\checkpoint-48\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8032733798027039, 'eval_precision': {'precision': 0.6041666666666666}, 'eval_recall': {'recall': 0.6274509803921569}, 'eval_f1': {'f1': 0.6141414141414141}, 'eval_runtime': 2.0462, 'eval_samples_per_second': 19.548, 'eval_steps_per_second': 1.466, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-48\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57233dfc782643ee93befc133b5b8a9c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-60\n","Configuration saved in ./results\\checkpoint-60\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8195002675056458, 'eval_precision': {'precision': 0.6041666666666666}, 'eval_recall': {'recall': 0.6274509803921569}, 'eval_f1': {'f1': 0.6141414141414141}, 'eval_runtime': 2.0352, 'eval_samples_per_second': 19.654, 'eval_steps_per_second': 1.474, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-60\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fdd96abff8a243958ddb3af632ad67fe","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-72\n","Configuration saved in ./results\\checkpoint-72\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8246606588363647, 'eval_precision': {'precision': 0.6041666666666666}, 'eval_recall': {'recall': 0.6274509803921569}, 'eval_f1': {'f1': 0.6141414141414141}, 'eval_runtime': 2.3457, 'eval_samples_per_second': 17.052, 'eval_steps_per_second': 1.279, 'epoch': 6.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-72\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e4a75cd29094746bc6cd9bf1ace9948","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-84\n","Configuration saved in ./results\\checkpoint-84\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.829282283782959, 'eval_precision': {'precision': 0.6041666666666666}, 'eval_recall': {'recall': 0.6274509803921569}, 'eval_f1': {'f1': 0.6141414141414141}, 'eval_runtime': 2.1024, 'eval_samples_per_second': 19.026, 'eval_steps_per_second': 1.427, 'epoch': 7.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-84\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54f35a24ea5042dbacc07f88df7d9f8e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-96\n","Configuration saved in ./results\\checkpoint-96\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8310612440109253, 'eval_precision': {'precision': 0.6041666666666666}, 'eval_recall': {'recall': 0.6274509803921569}, 'eval_f1': {'f1': 0.6141414141414141}, 'eval_runtime': 1.9631, 'eval_samples_per_second': 20.376, 'eval_steps_per_second': 1.528, 'epoch': 8.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-96\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-24 (score: 0.7627484202384949).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 570.8206, 'train_samples_per_second': 4.975, 'train_steps_per_second': 0.168, 'train_loss': 0.009004985292752584, 'epoch': 8.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f1810b2934a4333a2a982df6463b1f9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","\u001b[33m[W 2023-03-31 19:28:00,250]\u001b[0m Trial 0 failed with parameters: {'learning_rate': 2.369565540522295e-05, 'num_train_epochs': 8, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 16} because of the following error: The value {'f1': 0.6141414141414141} could not be cast to float..\u001b[0m\n","\u001b[33m[W 2023-03-31 19:28:00,252]\u001b[0m Trial 0 failed with value {'f1': 0.6141414141414141}.\u001b[0m\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 355\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 96\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88dcbccedfe1463ca3b0a54f15b575b0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/96 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f57551caf9db4ffeb9edf63a28d775de","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-12\n","Configuration saved in ./results\\checkpoint-12\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.1312284469604492, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.0122, 'eval_samples_per_second': 19.879, 'eval_steps_per_second': 1.491, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-12\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea8ae66ac5c94ad8826a39eee82b9bea","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-24\n","Configuration saved in ./results\\checkpoint-24\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.0062062740325928, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.8936, 'eval_samples_per_second': 21.123, 'eval_steps_per_second': 1.584, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-24\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6cf78977f0d14a05ab92491532cfeab1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-36\n","Configuration saved in ./results\\checkpoint-36\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.9598551988601685, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.237, 'eval_samples_per_second': 17.881, 'eval_steps_per_second': 1.341, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-36\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d94948e776bb438a9baedbc0b10177b4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.14166666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.19883040935672514}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-48\n","Configuration saved in ./results\\checkpoint-48\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.879197895526886, 'eval_precision': {'precision': 0.14166666666666666}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.19883040935672514}, 'eval_runtime': 1.8622, 'eval_samples_per_second': 21.479, 'eval_steps_per_second': 1.611, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-48\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9633adffd8748758846babff790f50d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-60\n","Configuration saved in ./results\\checkpoint-60\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8709257245063782, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.935, 'eval_samples_per_second': 20.672, 'eval_steps_per_second': 1.55, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-60\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78ff24c395404d808ac067b9d926d63c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-72\n","Configuration saved in ./results\\checkpoint-72\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8607368469238281, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.2975, 'eval_samples_per_second': 17.41, 'eval_steps_per_second': 1.306, 'epoch': 6.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-72\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f11535e5ffe4493cbf9f225ce76c9bc6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-84\n","Configuration saved in ./results\\checkpoint-84\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8562506437301636, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.271, 'eval_samples_per_second': 17.614, 'eval_steps_per_second': 1.321, 'epoch': 7.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-84\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da763e4763c940eca25cd343e816994c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-96\n","Configuration saved in ./results\\checkpoint-96\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8547385334968567, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.9711, 'eval_samples_per_second': 20.293, 'eval_steps_per_second': 1.522, 'epoch': 8.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-96\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-96 (score: 0.8547385334968567).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 593.8849, 'train_samples_per_second': 4.782, 'train_steps_per_second': 0.162, 'train_loss': 0.926463762919108, 'epoch': 8.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c174ae43a5c48238e6f71be3d723c2e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","\u001b[33m[W 2023-03-31 19:37:56,435]\u001b[0m Trial 1 failed with parameters: {'learning_rate': 0.0005694730236338067, 'num_train_epochs': 8, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 16} because of the following error: The value {'f1': 0.2295081967213115} could not be cast to float..\u001b[0m\n","\u001b[33m[W 2023-03-31 19:37:56,436]\u001b[0m Trial 1 failed with value {'f1': 0.2295081967213115}.\u001b[0m\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 355\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 42\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6b72741ce174a74874fa6ce1a402ccf","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/42 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97eaac2711cd4868af65b79c5952753a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-6\n","Configuration saved in ./results\\checkpoint-6\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8568235635757446, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.8746, 'eval_samples_per_second': 21.337, 'eval_steps_per_second': 0.533, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-6\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d853b09d1c464ade99421799fe774192","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-12\n","Configuration saved in ./results\\checkpoint-12\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8557740449905396, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.8683, 'eval_samples_per_second': 21.41, 'eval_steps_per_second': 0.535, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-12\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0bd9a9f29e0247f286bd99aa6f88a758","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-18\n","Configuration saved in ./results\\checkpoint-18\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8557353019714355, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.7298, 'eval_samples_per_second': 23.124, 'eval_steps_per_second': 0.578, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-18\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"efa55b750e8145dc8ae726a2f4405dea","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-24\n","Configuration saved in ./results\\checkpoint-24\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8559616804122925, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.6972, 'eval_samples_per_second': 23.568, 'eval_steps_per_second': 0.589, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-24\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"226181f48e584ccda7b0a1db444e1baa","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-30\n","Configuration saved in ./results\\checkpoint-30\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8560206294059753, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.9136, 'eval_samples_per_second': 20.903, 'eval_steps_per_second': 0.523, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-30\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf71843377244e8682aee1dd7cd29c8e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-36\n","Configuration saved in ./results\\checkpoint-36\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8560634851455688, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.7242, 'eval_samples_per_second': 23.199, 'eval_steps_per_second': 0.58, 'epoch': 6.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-36\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26baef544fe24287bfebc219d73dce89","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-42\n","Configuration saved in ./results\\checkpoint-42\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8560765981674194, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.8033, 'eval_samples_per_second': 22.181, 'eval_steps_per_second': 0.555, 'epoch': 7.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-42\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-18 (score: 0.8557353019714355).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 464.3503, 'train_samples_per_second': 5.352, 'train_steps_per_second': 0.09, 'train_loss': 0.8206965128580729, 'epoch': 7.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"77fa22a954374f29a1737870e75ead52","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","\u001b[33m[W 2023-03-31 19:45:42,840]\u001b[0m Trial 2 failed with parameters: {'learning_rate': 1.993986287245482e-05, 'num_train_epochs': 7, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 64} because of the following error: The value {'f1': 0.2295081967213115} could not be cast to float..\u001b[0m\n","\u001b[33m[W 2023-03-31 19:45:42,840]\u001b[0m Trial 2 failed with value {'f1': 0.2295081967213115}.\u001b[0m\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 355\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 60\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04c5eb75f54547b3bf6987ff09ee35e7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/60 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2dcee1b3c2134297b1d0d81359752239","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-12\n","Configuration saved in ./results\\checkpoint-12\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8631898760795593, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.9443, 'eval_samples_per_second': 20.573, 'eval_steps_per_second': 1.543, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-12\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2f3b826192841d89f479f9ba0397deb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-24\n","Configuration saved in ./results\\checkpoint-24\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8589441180229187, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.257, 'eval_samples_per_second': 17.723, 'eval_steps_per_second': 1.329, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-24\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd666ffb30f1420eaaee1a8a2b9b9ac8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-36\n","Configuration saved in ./results\\checkpoint-36\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8576846122741699, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.1723, 'eval_samples_per_second': 18.414, 'eval_steps_per_second': 1.381, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-36\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"247d4e49339e4e7c8faaeeb97a97e629","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-48\n","Configuration saved in ./results\\checkpoint-48\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8571740388870239, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.1402, 'eval_samples_per_second': 18.69, 'eval_steps_per_second': 1.402, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-48\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5881165a6c054df989547e5357b1f278","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-60\n","Configuration saved in ./results\\checkpoint-60\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8571009635925293, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.2238, 'eval_samples_per_second': 17.987, 'eval_steps_per_second': 1.349, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-60\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-60 (score: 0.8571009635925293).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 372.0796, 'train_samples_per_second': 4.77, 'train_steps_per_second': 0.161, 'train_loss': 0.7855982462565104, 'epoch': 5.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89636adbaa6a43cdb7cd2530c5a85179","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","\u001b[33m[W 2023-03-31 19:51:57,257]\u001b[0m Trial 3 failed with parameters: {'learning_rate': 4.711320860726576e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 16} because of the following error: The value {'f1': 0.2295081967213115} could not be cast to float..\u001b[0m\n","\u001b[33m[W 2023-03-31 19:51:57,258]\u001b[0m Trial 3 failed with value {'f1': 0.2295081967213115}.\u001b[0m\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 355\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 42\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62fb1b12455f4131b6ed583f3a41b325","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/42 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e56bac3e0bf4e5193f14b5b12c1ef65","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-6\n","Configuration saved in ./results\\checkpoint-6\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.859749972820282, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.1947, 'eval_samples_per_second': 18.226, 'eval_steps_per_second': 1.367, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-6\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3533b307d07a4a72be9d3fa9bfbb0a7d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-12\n","Configuration saved in ./results\\checkpoint-12\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8580020070075989, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.0206, 'eval_samples_per_second': 19.796, 'eval_steps_per_second': 1.485, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-12\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4af0e098026f4c3e86458b29d5f1e87b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-18\n","Configuration saved in ./results\\checkpoint-18\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8574946522712708, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.1722, 'eval_samples_per_second': 18.414, 'eval_steps_per_second': 1.381, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-18\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa1c4bc6350d4a65859cde5277f2a300","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-24\n","Configuration saved in ./results\\checkpoint-24\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8574539422988892, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.1429, 'eval_samples_per_second': 18.666, 'eval_steps_per_second': 1.4, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-24\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74d10c8e24d9452b994a46e1e7fba453","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-30\n","Configuration saved in ./results\\checkpoint-30\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8573075532913208, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.0913, 'eval_samples_per_second': 19.127, 'eval_steps_per_second': 1.434, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-30\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34464a8a488b43eca722e8cee5a2f0b2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-36\n","Configuration saved in ./results\\checkpoint-36\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8572273254394531, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.9425, 'eval_samples_per_second': 20.592, 'eval_steps_per_second': 1.544, 'epoch': 6.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-36\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"16ed97a5fa7648f19ea35fd2c30a1d01","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-42\n","Configuration saved in ./results\\checkpoint-42\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8572036027908325, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.0117, 'eval_samples_per_second': 19.884, 'eval_steps_per_second': 1.491, 'epoch': 7.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-42\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-42 (score: 0.8572036027908325).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 474.4099, 'train_samples_per_second': 5.238, 'train_steps_per_second': 0.089, 'train_loss': 0.8197848910377139, 'epoch': 7.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6172ee8262a4b988ddfc42d6031b7dd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","\u001b[33m[W 2023-03-31 19:59:53,831]\u001b[0m Trial 4 failed with parameters: {'learning_rate': 2.3557430735963788e-05, 'num_train_epochs': 7, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 16} because of the following error: The value {'f1': 0.2295081967213115} could not be cast to float..\u001b[0m\n","\u001b[33m[W 2023-03-31 19:59:53,832]\u001b[0m Trial 4 failed with value {'f1': 0.2295081967213115}.\u001b[0m\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 355\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 72\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee3bd1654f5a44378c65a087b96f0a23","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/72 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"294bb56124f746328a95512e0f628dce","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-12\n","Configuration saved in ./results\\checkpoint-12\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8608764410018921, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.8914, 'eval_samples_per_second': 21.148, 'eval_steps_per_second': 1.586, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-12\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a8957a50f3a4ab8bc47ce284015fc5e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-24\n","Configuration saved in ./results\\checkpoint-24\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8589082956314087, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.0857, 'eval_samples_per_second': 19.178, 'eval_steps_per_second': 1.438, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-24\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2448ed0eca14a789ae92439b0abeb19","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-36\n","Configuration saved in ./results\\checkpoint-36\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8585713505744934, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.2682, 'eval_samples_per_second': 17.635, 'eval_steps_per_second': 1.323, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-36\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d8898037d0d442a93773ba1873abc5a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-48\n","Configuration saved in ./results\\checkpoint-48\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8584084510803223, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.3244, 'eval_samples_per_second': 17.209, 'eval_steps_per_second': 1.291, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-48\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d684e961a484c698a1cd366c587a360","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-60\n","Configuration saved in ./results\\checkpoint-60\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.858178973197937, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.3843, 'eval_samples_per_second': 16.776, 'eval_steps_per_second': 1.258, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-60\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb6882237a8c4180b021a7a575e798a6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-72\n","Configuration saved in ./results\\checkpoint-72\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8580641746520996, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.6222, 'eval_samples_per_second': 15.254, 'eval_steps_per_second': 1.144, 'epoch': 6.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-72\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-72 (score: 0.8580641746520996).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 443.8768, 'train_samples_per_second': 4.799, 'train_steps_per_second': 0.162, 'train_loss': 0.7848748630947537, 'epoch': 6.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1df6cc108d34a7ca09091d112a3315d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","\u001b[33m[W 2023-03-31 20:07:20,311]\u001b[0m Trial 5 failed with parameters: {'learning_rate': 1.8175290785004167e-05, 'num_train_epochs': 6, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 16} because of the following error: The value {'f1': 0.2295081967213115} could not be cast to float..\u001b[0m\n","\u001b[33m[W 2023-03-31 20:07:20,312]\u001b[0m Trial 5 failed with value {'f1': 0.2295081967213115}.\u001b[0m\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 355\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 84\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6dfea5df5e747e09c684917bc4ad391","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/84 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6a6532adc6e4e6e8087aae6415071ad","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.14166666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.19883040935672514}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-12\n","Configuration saved in ./results\\checkpoint-12\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8762895464897156, 'eval_precision': {'precision': 0.14166666666666666}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.19883040935672514}, 'eval_runtime': 2.2622, 'eval_samples_per_second': 17.682, 'eval_steps_per_second': 0.884, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-12\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c9464e3e45944ba9a27860927ab1efe","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-24\n","Configuration saved in ./results\\checkpoint-24\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8541194200515747, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.1366, 'eval_samples_per_second': 18.722, 'eval_steps_per_second': 0.936, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-24\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"023a98cf924c4878a9e8cce9fca28ce8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-36\n","Configuration saved in ./results\\checkpoint-36\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8564978837966919, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.2937, 'eval_samples_per_second': 17.439, 'eval_steps_per_second': 0.872, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-36\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1a3c1fbf9e4416288cd24527ce5d88c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-48\n","Configuration saved in ./results\\checkpoint-48\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8599464297294617, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.2481, 'eval_samples_per_second': 17.793, 'eval_steps_per_second': 0.89, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-48\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97df338fae504d289c9dd9281e3aa2b6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-60\n","Configuration saved in ./results\\checkpoint-60\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8581017255783081, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.0217, 'eval_samples_per_second': 19.785, 'eval_steps_per_second': 0.989, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-60\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e21167a7c48d45aababfb22e8b2421c0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-72\n","Configuration saved in ./results\\checkpoint-72\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8583267331123352, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.0948, 'eval_samples_per_second': 19.095, 'eval_steps_per_second': 0.955, 'epoch': 6.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-72\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a632d32681d42b0bd7191195e820c70","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-84\n","Configuration saved in ./results\\checkpoint-84\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.857136070728302, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.7653, 'eval_samples_per_second': 22.66, 'eval_steps_per_second': 1.133, 'epoch': 7.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-84\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-24 (score: 0.8541194200515747).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 540.2488, 'train_samples_per_second': 4.6, 'train_steps_per_second': 0.155, 'train_loss': 0.7945499420166016, 'epoch': 7.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d928b432d5348d99ca3f18ad49dec9f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","\u001b[33m[W 2023-03-31 20:16:22,584]\u001b[0m Trial 6 failed with parameters: {'learning_rate': 0.0001379123986872968, 'num_train_epochs': 7, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32} because of the following error: The value {'f1': 0.2295081967213115} could not be cast to float..\u001b[0m\n","\u001b[33m[W 2023-03-31 20:16:22,585]\u001b[0m Trial 6 failed with value {'f1': 0.2295081967213115}.\u001b[0m\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 355\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 30\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d2ce4c337db49eeb05b4a060c704e18","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/30 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ccac7c6370154d81acda10f5c7fce90c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-6\n","Configuration saved in ./results\\checkpoint-6\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8730794191360474, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.9991, 'eval_samples_per_second': 20.009, 'eval_steps_per_second': 0.5, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-6\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b4dc9afcd27476b85b63c78b1810d76","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-12\n","Configuration saved in ./results\\checkpoint-12\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8522273898124695, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.9741, 'eval_samples_per_second': 20.263, 'eval_steps_per_second': 0.507, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-12\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cf8735c004c4b498004b817f804a9dc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-18\n","Configuration saved in ./results\\checkpoint-18\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8556569814682007, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.7484, 'eval_samples_per_second': 22.878, 'eval_steps_per_second': 0.572, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-18\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da45ef3d0332425c86e209700e4bb180","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-24\n","Configuration saved in ./results\\checkpoint-24\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8639097213745117, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.7018, 'eval_samples_per_second': 23.504, 'eval_steps_per_second': 0.588, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-24\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"03435635f3ed4007ace396d97af2637b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-30\n","Configuration saved in ./results\\checkpoint-30\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.862366795539856, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.8943, 'eval_samples_per_second': 21.117, 'eval_steps_per_second': 0.528, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-30\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-12 (score: 0.8522273898124695).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 64\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 328.1875, 'train_samples_per_second': 5.408, 'train_steps_per_second': 0.091, 'train_loss': 0.8289403915405273, 'epoch': 5.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c3aa8819b14844bf86cd9110f2e0d11a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","\u001b[33m[W 2023-03-31 20:21:52,739]\u001b[0m Trial 7 failed with parameters: {'learning_rate': 0.00036934112948531806, 'num_train_epochs': 5, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 64} because of the following error: The value {'f1': 0.2295081967213115} could not be cast to float..\u001b[0m\n","\u001b[33m[W 2023-03-31 20:21:52,740]\u001b[0m Trial 7 failed with value {'f1': 0.2295081967213115}.\u001b[0m\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 355\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 36\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53f6a1d5703345bda2af55369f960255","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/36 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"997a131e64504f409c059f1fffee31a0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-6\n","Configuration saved in ./results\\checkpoint-6\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8546785116195679, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.7932, 'eval_samples_per_second': 22.307, 'eval_steps_per_second': 1.115, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-6\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"954b41d7ff73410c991efc438aede472","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-12\n","Configuration saved in ./results\\checkpoint-12\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8569850921630859, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.1438, 'eval_samples_per_second': 18.659, 'eval_steps_per_second': 0.933, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-12\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0eb4091d82e40c8b57f76c8487165cc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-18\n","Configuration saved in ./results\\checkpoint-18\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8568398356437683, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.4607, 'eval_samples_per_second': 27.384, 'eval_steps_per_second': 1.369, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-18\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f3233e047214364bfd2b96e9e33a5c2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-24\n","Configuration saved in ./results\\checkpoint-24\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8569937944412231, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.9774, 'eval_samples_per_second': 20.229, 'eval_steps_per_second': 1.011, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-24\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1c34040a940446e9c21b62be53ffc48","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-30\n","Configuration saved in ./results\\checkpoint-30\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8572376370429993, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.8597, 'eval_samples_per_second': 21.509, 'eval_steps_per_second': 1.075, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-30\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe465ddbfe49473e9ff3ba9e83525f6f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-36\n","Configuration saved in ./results\\checkpoint-36\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.857298731803894, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.0209, 'eval_samples_per_second': 19.793, 'eval_steps_per_second': 0.99, 'epoch': 6.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-36\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-6 (score: 0.8546785116195679).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 32\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 403.8124, 'train_samples_per_second': 5.275, 'train_steps_per_second': 0.089, 'train_loss': 0.8099691602918837, 'epoch': 6.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d126da5e80704f7e96c47148baa7b607","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","\u001b[33m[W 2023-03-31 20:28:38,581]\u001b[0m Trial 8 failed with parameters: {'learning_rate': 4.8505626358579326e-05, 'num_train_epochs': 6, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 32} because of the following error: The value {'f1': 0.2295081967213115} could not be cast to float..\u001b[0m\n","\u001b[33m[W 2023-03-31 20:28:38,582]\u001b[0m Trial 8 failed with value {'f1': 0.2295081967213115}.\u001b[0m\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","c:\\Users\\danid\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 355\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 84\n","  Number of trainable parameters = 134902275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9169f0fa732f42929d538b6b5d6609bd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/84 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"008298f076e64b448cd6e10df4810e47","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.14166666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.19883040935672514}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-12\n","Configuration saved in ./results\\checkpoint-12\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8652013540267944, 'eval_precision': {'precision': 0.14166666666666666}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.19883040935672514}, 'eval_runtime': 1.9491, 'eval_samples_per_second': 20.522, 'eval_steps_per_second': 1.539, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-12\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc5ab83691f34638a3b15415c0d4e8a0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-24\n","Configuration saved in ./results\\checkpoint-24\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8657451868057251, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.8639, 'eval_samples_per_second': 21.46, 'eval_steps_per_second': 1.61, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-24\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18ea10b0ae534847bda285d26b8b121f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-36\n","Configuration saved in ./results\\checkpoint-36\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8716751933097839, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.7775, 'eval_samples_per_second': 22.503, 'eval_steps_per_second': 1.688, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-36\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50155231419b4a6d9c811934c2a31557","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-48\n","Configuration saved in ./results\\checkpoint-48\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.885004997253418, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.8708, 'eval_samples_per_second': 21.381, 'eval_steps_per_second': 1.604, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-48\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d31891b9e1294332936c33d4658a6d55","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.14166666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.19883040935672514}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-60\n","Configuration saved in ./results\\checkpoint-60\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.892661452293396, 'eval_precision': {'precision': 0.14166666666666666}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.19883040935672514}, 'eval_runtime': 1.8999, 'eval_samples_per_second': 21.054, 'eval_steps_per_second': 1.579, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-60\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e2a0bf632ea4bc38b3de12b4b7c2732","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-72\n","Configuration saved in ./results\\checkpoint-72\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8626613616943359, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 1.8249, 'eval_samples_per_second': 21.919, 'eval_steps_per_second': 1.644, 'epoch': 6.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-72\\pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed710340fc894753be41df9ca8c11576","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Saving model checkpoint to ./results\\checkpoint-84\n","Configuration saved in ./results\\checkpoint-84\\config.json\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8558170199394226, 'eval_precision': {'precision': 0.17500000000000002}, 'eval_recall': {'recall': 0.3333333333333333}, 'eval_f1': {'f1': 0.2295081967213115}, 'eval_runtime': 2.0194, 'eval_samples_per_second': 19.808, 'eval_steps_per_second': 1.486, 'epoch': 7.0}\n"]},{"name":"stderr","output_type":"stream","text":["Model weights saved in ./results\\checkpoint-84\\pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./results\\checkpoint-84 (score: 0.8558170199394226).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 40\n","  Batch size = 16\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 498.8641, 'train_samples_per_second': 4.981, 'train_steps_per_second': 0.168, 'train_loss': 0.8337012699672154, 'epoch': 7.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"109a006e3e4d4c0d92984b786c7c59a0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.17500000000000002}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.3333333333333333}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.2295081967213115}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","\u001b[33m[W 2023-03-31 20:36:59,632]\u001b[0m Trial 9 failed with parameters: {'learning_rate': 0.0008073273138029238, 'num_train_epochs': 7, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 16} because of the following error: The value {'f1': 0.2295081967213115} could not be cast to float..\u001b[0m\n","\u001b[33m[W 2023-03-31 20:36:59,633]\u001b[0m Trial 9 failed with value {'f1': 0.2295081967213115}.\u001b[0m\n"]}],"source":["import optuna\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    # Define the hyperparameters to optimize\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n","    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 5, 10)\n","    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [32, 64])\n","    per_device_eval_batch_size = trial.suggest_categorical(\"per_device_eval_batch_size\", [16, 32, 64])\n","\n","    # Define the training arguments\n","    training_args = TrainingArguments(\n","        output_dir='./results',\n","        evaluation_strategy='epoch',\n","        save_strategy='epoch',\n","        learning_rate=learning_rate,\n","        per_device_train_batch_size=per_device_train_batch_size,\n","        per_device_eval_batch_size=per_device_eval_batch_size,\n","        num_train_epochs=num_train_epochs,\n","        logging_dir='./logs',\n","        load_best_model_at_end=True,\n","        seed=1\n","    )\n","\n","    # Create a Trainer instance\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        compute_metrics=compute_metrics\n","    )\n","\n","    # Fine-tune the model on the training dataset\n","    trainer.train()\n","\n","    # Evaluate the model on the validation dataset\n","    eval_result = trainer.evaluate(eval_dataset=val_dataset)\n","\n","    return eval_result[\"eval_f1\"]\n","\n","# Create a study object and optimize the hyperparameters\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=10)\n"]},{"cell_type":"markdown","metadata":{"id":"e3pJWfBxJvMz"},"source":["We can now call the `best_params` argument and evaluate the model on the test set. Because of some kernel crashes, I'll manually input in the code below the parameters that yielded better results from this tuning. "]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a7503d15567c46539c3f640c065f2616","0062b66867584bf98935e980a7730245","0e0b2153bf2441adb4423a21d780e8e5","c8992d419a204efa915320720173196e","b815f608666148218fd428bc9b4bcf7f","cdbd1b14eef745fda7e89a2ccafd0edb","99d3f3b04f7e4d49bf7ac5018ea4ad78","6d29c48636df4ff382e96e86e8cda260","20070397706b4871bc9714288ff62a49","34d42a35aa96466198c37f0e8691d9f2","052234e8367e4e1b91fc73a15848e3f7","db06f397ef2046f0a48092b1845c1f5e","5d8ed021962b41c79c3f146f1fd6494b","976638cbeb3749e287371f6c7cb6cdda","c26a5f39ea4248848e85cbb37cf57fc6","c5281c4374f1495d992a71326c8c10b8","8798baa7952c41e38aee98e793f97455","02a860a70d6b408daa6cd5a1dde847b8","0cba20e3cfd44868aa9ae07a545f1d0b","9b54eeaa4747433cacef0bebd1f48c65","630cfa3b18484902941efe2ed8ff6a8a","3fade6c9457e4ada9c06b4db7c589568","f8ef2cd6e0db414681f6e3aba9e7690c","b6131db9078c4cafb2492e64e876b37b","7c5a3c878adf4db1a768fa9f2cc53ce0","8893964ac9cc4222bfe59189883c4530","69918acf6a084daeb8615ba0970fc225","caa4b138fc9c4b7eb771bafab0b28036","4502f8239458423b8b9cdd0efd704007","17f778f22bcd4c8da8758a5b057a9512","2826b056637c43a99e4df7c6fbf6318e","a5eddfc55a00484fa9c4deb6098d1ce0","cd054ff332af4057968cdd90c6b07cd5"]},"executionInfo":{"elapsed":1584344,"status":"error","timestamp":1680292599551,"user":{"displayName":"Daniela De los Santos","userId":"01341847845262557810"},"user_tz":-120},"id":"XaW6JbGUGL6f","outputId":"50547461-7895-48bd-eeef-be4601fc4e71"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a7503d15567c46539c3f640c065f2616","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/355 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db06f397ef2046f0a48092b1845c1f5e","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/40 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8ef2cd6e0db414681f6e3aba9e7690c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/169 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [96/96 25:30, Epoch 8/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.492835</td>\n","      <td>{'precision': 0.5498721227621483}</td>\n","      <td>{'recall': 0.5760971055088703}</td>\n","      <td>{'f1': 0.5623885918003565}</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.446478</td>\n","      <td>{'precision': 0.5488721804511277}</td>\n","      <td>{'recall': 0.5798319327731093}</td>\n","      <td>{'f1': 0.5634920634920634}</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.339875</td>\n","      <td>{'precision': 0.5993265993265994}</td>\n","      <td>{'recall': 0.6311858076563959}</td>\n","      <td>{'f1': 0.6148394241417497}</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.493289</td>\n","      <td>{'precision': 0.5488721804511277}</td>\n","      <td>{'recall': 0.5798319327731093}</td>\n","      <td>{'f1': 0.5634920634920634}</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>0.694449</td>\n","      <td>{'precision': 0.566951566951567}</td>\n","      <td>{'recall': 0.5686274509803922}</td>\n","      <td>{'f1': 0.5583333333333335}</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>No log</td>\n","      <td>0.458407</td>\n","      <td>{'precision': 0.6041666666666666}</td>\n","      <td>{'recall': 0.6274509803921569}</td>\n","      <td>{'f1': 0.6141414141414141}</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>No log</td>\n","      <td>0.473179</td>\n","      <td>{'precision': 0.6041666666666666}</td>\n","      <td>{'recall': 0.6274509803921569}</td>\n","      <td>{'f1': 0.6141414141414141}</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>No log</td>\n","      <td>0.476864</td>\n","      <td>{'precision': 0.6041666666666666}</td>\n","      <td>{'recall': 0.6274509803921569}</td>\n","      <td>{'f1': 0.6141414141414141}</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.5498721227621483}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.5760971055088703}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.5623885918003565}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.5488721804511277}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.5798319327731093}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.5634920634920634}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.5993265993265994}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6311858076563959}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6148394241417497}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.5488721804511277}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.5798319327731093}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.5634920634920634}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.566951566951567}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.5686274509803922}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.5583333333333335}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Trainer is attempting to log a value of \"{'precision': 0.6041666666666666}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'recall': 0.6274509803921569}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","Trainer is attempting to log a value of \"{'f1': 0.6141414141414141}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-0965216de3b7>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Compute the classification report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Print the classification report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'classification_report' is not defined"]}],"source":["# Import dataset\n","train_dataset, val_dataset, test_dataset = raw_reader_and_tokenize(\"climate\")\n","\n","best_training_args = TrainingArguments(\n","    output_dir='./results',\n","    evaluation_strategy='epoch',\n","    save_strategy='epoch',\n","    learning_rate=1e-4,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=8,\n","    logging_dir='./logs',\n","    load_best_model_at_end=True,\n","    seed=1\n",")\n","\n","# Create a Trainer instance\n","trainer = Trainer(\n","    model=model,\n","    args=best_training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Fine-tune the model on the training dataset\n","trainer.train()\n","\n","# Generate predictions for the test set\n","test_predictions = trainer.predict(test_dataset).predictions\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","\n"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":683,"status":"ok","timestamp":1680292783806,"user":{"displayName":"Daniela De los Santos","userId":"01341847845262557810"},"user_tz":-120},"id":"Iega3vghPtE0","outputId":"2ddd5baf-5557-48ff-ceb7-f5e00ff35171"},"outputs":[{"name":"stdout","output_type":"stream","text":["Class: 0\n","Precision: 0.6595744680851063\n","Recall: 0.8857142857142857\n","F1-score: 0.7560975609756098\n","\n","Class: 1\n","Precision: 0.0\n","Recall: 0.0\n","F1-score: 0.0\n","\n","Class: 2\n","Precision: 0.8934426229508197\n","Recall: 0.8861788617886179\n","F1-score: 0.8897959183673471\n","\n","Class: macro avg\n","Precision: 0.517672363678642\n","Recall: 0.5906310491676345\n","F1-score: 0.5486311597809856\n","\n","Class: weighted avg\n","Precision: 0.7868553195617133\n","Recall: 0.8284023668639053\n","F1-score: 0.8041911987771008\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["# Compute the classification report\n","report = classification_report(test_dataset['label'], test_predictions, output_dict=True)\n","\n","# Print the classification report\n","for label in report:\n","    if label != 'accuracy':\n","        print(f\"Class: {label}\")\n","        print(f\"Precision: {report[label]['precision']}\")\n","        print(f\"Recall: {report[label]['recall']}\")\n","        print(f\"F1-score: {report[label]['f1-score']}\\n\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Even after hyperparameter tuning, the results do not improve substantially for the climate dataset."]},{"cell_type":"markdown","metadata":{"id":"FpGMKxZUNRwr"},"source":["## Where is our model failing?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RUVx8qpfTZm0"},"source":["We can look at some examples of where the model fails. Let's take, again, the climate dataset."]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":168,"status":"ok","timestamp":1680293311800,"user":{"displayName":"Daniela De los Santos","userId":"01341847845262557810"},"user_tz":-120},"id":"PsP8PKSIQjTA"},"outputs":[],"source":["climate_outcomes = pd.DataFrame(test_dataset[\"text\"])\n","climate_outcomes[\"label_real\"]= test_dataset[\"label\"]\n","climate_outcomes[\"label_predicted\"]= test_predictions\n","climate_outcomes[\"wrong_prediction\"]= climate_outcomes[\"label_predicted\"]!=climate_outcomes[\"label_real\"]\n","wrong_outcomes = climate_outcomes[climate_outcomes[\"wrong_prediction\"]==True]\n"]},{"cell_type":"code","execution_count":84,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":959},"executionInfo":{"elapsed":590,"status":"ok","timestamp":1680294379894,"user":{"displayName":"Daniela De los Santos","userId":"01341847845262557810"},"user_tz":-120},"id":"BHPk9QMPRkvw","outputId":"2a41a4b9-b2fe-4691-cf6c-0e91d6d8d106"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-88f12ec7-c9b1-47b3-b793-d238fc98e2f1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>label_real</th>\n","      <th>label_predicted</th>\n","      <th>wrong_prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>12</th>\n","      <td>grow food not lawns  Lawns are Ecocide in your yard. #Patriarchy is Ecocide of your own species #DominoEffect #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Citizens have a right to access information that would protect their lives against #risk #CLIMWARN #CFCC15 #vulnerability #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>It's raining and feels like fall I don't know what kinda summer this is  #isReal #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>As Western Canada Burns Harper's got his head so deep in the #tarsands he can see #ChristyClark's ankles #BCwildfire #cdnpoli #SemST</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>Turn the Liverpool Plains into a coal mine...is there any bad idea this government doesn't like? #FoodBowl #Shenhua #auspol #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>The Weather app keeps taunting us with rain. #PNW #drought #SemST</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>Haven't moved an inch since yesterday.  #bcpoli #vancouver #bcwildfire #yvr #realrenewables #SemST</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>@user tackles difficult challenges of growth needs and unemployment alongside #CFCC15. #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>85</th>\n","      <td>@user @user @user @user @user Criminal capitalist wrecked economy08Now misleadingInvestors as to Risk of #Ecocide #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>87</th>\n","      <td>@user @user @user @user #CSOTA #fossilfuels corp misleading investors about liability for #Ecocide #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>What has happened to sunny Colorado?? #cloudy #denver #nosunshine #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>ONE Volcano emits more pollution than man has in our HISTORY! #SemST</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>109</th>\n","      <td>@user ice cracking in the summer?? SO ALARMING. #climatechangehoax #SemST</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>114</th>\n","      <td>67 degrees Fahrenheit on July 8th. I love this weather! #PureMichigan #SemST</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>120</th>\n","      <td>#ClimateChangeDefinitions \"settled science\" = flawed computer models with jiggered data to force a predefined conclusion. #SemST</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>125</th>\n","      <td>What's next after 2 years? :) #thinkbig #world #solution #water #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>127</th>\n","      <td>@user Still a larval theory. Still poorly modelled. Still unquantified. Still multi-factor. Still flummoxed by albedo. #SemST</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>136</th>\n","      <td>Climate deniers is a term used to silence those pointing out the hypocrisy in the fanatical zeal on  #climatetruth #SemST</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>138</th>\n","      <td>@user Will we change? That's the reason we are here. Be of good cheer. We're going to win this #CSOTA #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>@user yes ironic that the alarmists are the actual \"deniers #SemST</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>146</th>\n","      <td>@user Hey Justin I will give you 50 cents if you stop talking about climate 'Change' #Ottawa #davidsuzuki #cbc #SemST</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>147</th>\n","      <td>If @user &amp; @user don't have to follow SCOTUS why should anyone else have to #ClimateFraud #SpecialReport #WakeUpAmerica #SemST</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>If we want a future for our/Canadian kids we all need to seriously wake the fuck up,No more talk only action #Harper  #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>Govt announcement on zero carbon homes \"short-sighted, unnecessary, retrograde and damaging\" says @user #energyefficiency #SemST</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>151</th>\n","      <td>We are not \"killing the Earth\". The Earth has been through worse and will be fine after all humans suffocate, drown or starve #SemST</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>153</th>\n","      <td>@user This is not a fantasy this is negligence collusion with criminal corporations acting with negligence to #Ecocide #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>154</th>\n","      <td>We need governments and corporations to respond AND ordinary people to change our daily habits to consume less. #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>157</th>\n","      <td>The only thing \"man made\" about global warming is the false narrative. #WakeUpAmerica #boycottSanFrancisco #Election2016 #SemST</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>166</th>\n","      <td>If we touch #Antarctica for one more #reason other than to #research the #impact of us we would be #insane! #Democracy #Goal? #SemST</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88f12ec7-c9b1-47b3-b793-d238fc98e2f1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-88f12ec7-c9b1-47b3-b793-d238fc98e2f1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-88f12ec7-c9b1-47b3-b793-d238fc98e2f1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                                                                                                         0  \\\n","12                   grow food not lawns  Lawns are Ecocide in your yard. #Patriarchy is Ecocide of your own species #DominoEffect #SemST    \n","15       Citizens have a right to access information that would protect their lives against #risk #CLIMWARN #CFCC15 #vulnerability #SemST    \n","23                                                It's raining and feels like fall I don't know what kinda summer this is  #isReal #SemST    \n","29   As Western Canada Burns Harper's got his head so deep in the #tarsands he can see #ChristyClark's ankles #BCwildfire #cdnpoli #SemST    \n","43     Turn the Liverpool Plains into a coal mine...is there any bad idea this government doesn't like? #FoodBowl #Shenhua #auspol #SemST    \n","46                                                                      The Weather app keeps taunting us with rain. #PNW #drought #SemST    \n","49                                     Haven't moved an inch since yesterday.  #bcpoli #vancouver #bcwildfire #yvr #realrenewables #SemST    \n","65                                          @user tackles difficult challenges of growth needs and unemployment alongside #CFCC15. #SemST    \n","85               @user @user @user @user @user Criminal capitalist wrecked economy08Now misleadingInvestors as to Risk of #Ecocide #SemST    \n","87                              @user @user @user @user #CSOTA #fossilfuels corp misleading investors about liability for #Ecocide #SemST    \n","96                                                               What has happened to sunny Colorado?? #cloudy #denver #nosunshine #SemST    \n","99                                                                   ONE Volcano emits more pollution than man has in our HISTORY! #SemST    \n","109                                                             @user ice cracking in the summer?? SO ALARMING. #climatechangehoax #SemST    \n","114                                                          67 degrees Fahrenheit on July 8th. I love this weather! #PureMichigan #SemST    \n","120      #ClimateChangeDefinitions \"settled science\" = flawed computer models with jiggered data to force a predefined conclusion. #SemST    \n","125                                                                What's next after 2 years? :) #thinkbig #world #solution #water #SemST    \n","127         @user Still a larval theory. Still poorly modelled. Still unquantified. Still multi-factor. Still flummoxed by albedo. #SemST    \n","136             Climate deniers is a term used to silence those pointing out the hypocrisy in the fanatical zeal on  #climatetruth #SemST    \n","138                          @user Will we change? That's the reason we are here. Be of good cheer. We're going to win this #CSOTA #SemST    \n","145                                                                    @user yes ironic that the alarmists are the actual \"deniers #SemST    \n","146                 @user Hey Justin I will give you 50 cents if you stop talking about climate 'Change' #Ottawa #davidsuzuki #cbc #SemST    \n","147        If @user & @user don't have to follow SCOTUS why should anyone else have to #ClimateFraud #SpecialReport #WakeUpAmerica #SemST    \n","148          If we want a future for our/Canadian kids we all need to seriously wake the fuck up,No more talk only action #Harper  #SemST    \n","149      Govt announcement on zero carbon homes \"short-sighted, unnecessary, retrograde and damaging\" says @user #energyefficiency #SemST    \n","151  We are not \"killing the Earth\". The Earth has been through worse and will be fine after all humans suffocate, drown or starve #SemST    \n","153         @user This is not a fantasy this is negligence collusion with criminal corporations acting with negligence to #Ecocide #SemST    \n","154                We need governments and corporations to respond AND ordinary people to change our daily habits to consume less. #SemST    \n","157       The only thing \"man made\" about global warming is the false narrative. #WakeUpAmerica #boycottSanFrancisco #Election2016 #SemST    \n","166  If we touch #Antarctica for one more #reason other than to #research the #impact of us we would be #insane! #Democracy #Goal? #SemST    \n","\n","     label_real  label_predicted  wrong_prediction  \n","12            2                0              True  \n","15            2                0              True  \n","23            2                0              True  \n","29            0                2              True  \n","43            2                0              True  \n","46            0                2              True  \n","49            0                2              True  \n","65            2                0              True  \n","85            2                0              True  \n","87            2                0              True  \n","96            2                0              True  \n","99            1                2              True  \n","109           1                2              True  \n","114           0                2              True  \n","120           1                2              True  \n","125           2                0              True  \n","127           1                0              True  \n","136           1                2              True  \n","138           2                0              True  \n","145           1                0              True  \n","146           1                2              True  \n","147           1                2              True  \n","148           2                0              True  \n","149           1                2              True  \n","151           1                2              True  \n","153           2                0              True  \n","154           2                0              True  \n","157           1                2              True  \n","166           2                0              True  "]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["pd.set_option('max_colwidth', 200)\n","\n","wrong_outcomes"]},{"cell_type":"markdown","metadata":{"id":"sHh5pKzWTfn4"},"source":["We see some examples of misclassification as 'against' (2) where the correct label is 'in favor' (1), which seem reasonable. For instance, in line 136 the bigram 'climate deniers' is probably helping the classifier lean towards 'in favor', as it is a typical way to critize those against climate change narrative. However, the intention of the tweet is to critize the use of the term, which confuses the algorithm.\n","\n","Other cases are less clear: it would be reasonable to have them correctly classied - such is the case of line 147, where the hashtag #ClimateFreud is used.\n","\n","As we can see in the table below, the main misclasifications happen following this logic:\n","Neutral tweets classified as 'against'\n","'Against' tweets classified as neutral\n","'Against tweets classified as 'In favor'.\n","\n"]},{"cell_type":"code","execution_count":90,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680294473528,"user":{"displayName":"Daniela De los Santos","userId":"01341847845262557810"},"user_tz":-120},"id":"_j9RFulmUtPl","outputId":"d0df320c-301a-4436-ecc4-a0ab28152fa4"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-2375c0b9-141e-4382-8f24-0cd9096814a0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>label_predicted</th>\n","      <th>0</th>\n","      <th>2</th>\n","    </tr>\n","    <tr>\n","      <th>label_real</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.181818</td>\n","      <td>0.818182</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2375c0b9-141e-4382-8f24-0cd9096814a0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2375c0b9-141e-4382-8f24-0cd9096814a0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2375c0b9-141e-4382-8f24-0cd9096814a0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["label_predicted         0         2\n","label_real                         \n","0                0.000000  1.000000\n","1                0.181818  0.818182\n","2                1.000000  0.000000"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["pd.crosstab(wrong_outcomes['label_real'], wrong_outcomes['label_predicted'], normalize='index')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"a9bf44a63aa85fc488ba2c18070365b0fdf586414cbab845f059fd6426d069d2"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"0062b66867584bf98935e980a7730245":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdbd1b14eef745fda7e89a2ccafd0edb","placeholder":"â€‹","style":"IPY_MODEL_99d3f3b04f7e4d49bf7ac5018ea4ad78","value":"Map: 100%"}},"02a860a70d6b408daa6cd5a1dde847b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"052234e8367e4e1b91fc73a15848e3f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0cba20e3cfd44868aa9ae07a545f1d0b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e0b2153bf2441adb4423a21d780e8e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d29c48636df4ff382e96e86e8cda260","max":355,"min":0,"orientation":"horizontal","style":"IPY_MODEL_20070397706b4871bc9714288ff62a49","value":355}},"17f778f22bcd4c8da8758a5b057a9512":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20070397706b4871bc9714288ff62a49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2826b056637c43a99e4df7c6fbf6318e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"34d42a35aa96466198c37f0e8691d9f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fade6c9457e4ada9c06b4db7c589568":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4502f8239458423b8b9cdd0efd704007":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d8ed021962b41c79c3f146f1fd6494b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8798baa7952c41e38aee98e793f97455","placeholder":"â€‹","style":"IPY_MODEL_02a860a70d6b408daa6cd5a1dde847b8","value":"Map:   0%"}},"630cfa3b18484902941efe2ed8ff6a8a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69918acf6a084daeb8615ba0970fc225":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"6d29c48636df4ff382e96e86e8cda260":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c5a3c878adf4db1a768fa9f2cc53ce0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_17f778f22bcd4c8da8758a5b057a9512","max":169,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2826b056637c43a99e4df7c6fbf6318e","value":169}},"8798baa7952c41e38aee98e793f97455":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8893964ac9cc4222bfe59189883c4530":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5eddfc55a00484fa9c4deb6098d1ce0","placeholder":"â€‹","style":"IPY_MODEL_cd054ff332af4057968cdd90c6b07cd5","value":" 169/169 [00:00&lt;00:00, 1632.37 examples/s]"}},"976638cbeb3749e287371f6c7cb6cdda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cba20e3cfd44868aa9ae07a545f1d0b","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9b54eeaa4747433cacef0bebd1f48c65","value":40}},"99d3f3b04f7e4d49bf7ac5018ea4ad78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b54eeaa4747433cacef0bebd1f48c65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a5eddfc55a00484fa9c4deb6098d1ce0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7503d15567c46539c3f640c065f2616":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0062b66867584bf98935e980a7730245","IPY_MODEL_0e0b2153bf2441adb4423a21d780e8e5","IPY_MODEL_c8992d419a204efa915320720173196e"],"layout":"IPY_MODEL_b815f608666148218fd428bc9b4bcf7f"}},"b6131db9078c4cafb2492e64e876b37b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_caa4b138fc9c4b7eb771bafab0b28036","placeholder":"â€‹","style":"IPY_MODEL_4502f8239458423b8b9cdd0efd704007","value":"Map: 100%"}},"b815f608666148218fd428bc9b4bcf7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"c26a5f39ea4248848e85cbb37cf57fc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_630cfa3b18484902941efe2ed8ff6a8a","placeholder":"â€‹","style":"IPY_MODEL_3fade6c9457e4ada9c06b4db7c589568","value":" 0/40 [00:00&lt;?, ? examples/s]"}},"c5281c4374f1495d992a71326c8c10b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"c8992d419a204efa915320720173196e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34d42a35aa96466198c37f0e8691d9f2","placeholder":"â€‹","style":"IPY_MODEL_052234e8367e4e1b91fc73a15848e3f7","value":" 355/355 [00:00&lt;00:00, 797.49 examples/s]"}},"caa4b138fc9c4b7eb771bafab0b28036":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd054ff332af4057968cdd90c6b07cd5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cdbd1b14eef745fda7e89a2ccafd0edb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db06f397ef2046f0a48092b1845c1f5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5d8ed021962b41c79c3f146f1fd6494b","IPY_MODEL_976638cbeb3749e287371f6c7cb6cdda","IPY_MODEL_c26a5f39ea4248848e85cbb37cf57fc6"],"layout":"IPY_MODEL_c5281c4374f1495d992a71326c8c10b8"}},"f8ef2cd6e0db414681f6e3aba9e7690c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6131db9078c4cafb2492e64e876b37b","IPY_MODEL_7c5a3c878adf4db1a768fa9f2cc53ce0","IPY_MODEL_8893964ac9cc4222bfe59189883c4530"],"layout":"IPY_MODEL_69918acf6a084daeb8615ba0970fc225"}}}}},"nbformat":4,"nbformat_minor":0}
