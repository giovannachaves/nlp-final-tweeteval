{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install deps\n",
    "# !python -m spacy download en_core_web_md\n",
    "# !pip install torch torchvision\n",
    "# !pip install transformers\n",
    "# !pip3 install nltk emoji==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#visualize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# modeling\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader_df(topic):\n",
    "  path_train = \"cleaned_df/stance_\" + topic + \"_train_cleaned.csv\"\n",
    "  path_test = \"cleaned_df/stance_\" + topic + \"_test_cleaned.csv\"\n",
    "  path_val = \"cleaned_df/stance_\" + topic + \"_validation_cleaned.csv\"\n",
    "  df_train = pd.read_csv(path_train)\n",
    "  df_val = pd.read_csv(path_val)\n",
    "  df_test = pd.read_csv(path_test)\n",
    "\n",
    "  X_train = df_train.loc[:, 'text'].values\n",
    "  y_train = df_train.loc[:, 'label'].values\n",
    "\n",
    "  X_test = df_test.loc[:, 'text'].values\n",
    "  y_test = df_test.loc[:, 'label'].values\n",
    "\n",
    "  X_val = df_val.loc[:, 'text'].values\n",
    "  y_val = df_val.loc[:, 'label'].values\n",
    "\n",
    "  return X_train, X_test, y_train, y_test, X_val, y_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the metrics we want to call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_trues, y_preds, verbose=True):\n",
    "\n",
    "  recall = recall_score(y_trues, y_preds, average='weighted') * 100\n",
    "  precision = precision_score(y_trues, y_preds, average='weighted') * 100\n",
    "\n",
    "  if verbose:\n",
    "    print(f'Precision: {precision:.2f}')\n",
    "    print(f'Recall: {recall:.2f}')\n",
    "\n",
    "  return recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the batches\n",
    "\n",
    "def get_batches(X_train, y_train, tokenizer, batch_size, max_length):\n",
    "    \"\"\"\n",
    "    Objective: from features and labels yield a random batch of batch_size of (features, labels),\n",
    "               each time we reached all data we shuffle again the (features, labels) \n",
    "               and we do it again (infinite loop)\n",
    "\n",
    "    Inputs:\n",
    "        - X_train, np.array: the texts (features)\n",
    "        - y_train, np.array: the labels\n",
    "        - tokenizer, transformers.tokenization_distilbert.DistilBertTokenizer: the tokenizer of the model\n",
    "        - batch_size, int: the size of the batch we yield\n",
    "        - max_length, int: the input shape of the data\n",
    "    Outputs: (generator)\n",
    "        - inputs, np.array : two arrays one with ids from the tokenizer, and the masks associated with the padding\n",
    "        - targets, np.array: the label array of the associated inputs\n",
    "    \"\"\"\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=11)\n",
    "\n",
    "    i, j = 0, 0\n",
    "\n",
    "    while i > -1:\n",
    "\n",
    "        if (len(X_train) - j*batch_size) < batch_size:\n",
    "            j = 0\n",
    "            X_train, y_train = shuffle(X_train, y_train, random_state=11)\n",
    "\n",
    "        sentences = X_train[j*batch_size: (j+1) * batch_size]\n",
    "        targets = y_train[j*batch_size: (j+1) * batch_size, :]\n",
    "        j += 1\n",
    "\n",
    "        input_ids, input_masks = [],[]\n",
    "\n",
    "        # see if puting following before the loop may improve the training in time and RAM used\n",
    "        inputs = tokenizer.batch_encode_plus(list(sentences), add_special_tokens=True, max_length=max_length, \n",
    "                                            padding='max_length',  return_attention_mask=True,\n",
    "                                            return_token_type_ids=True, truncation=True,\n",
    "                                             return_tensors=\"np\")\n",
    "\n",
    "        ids = np.asarray(inputs['input_ids'], dtype='int32')\n",
    "        masks = np.asarray(inputs['attention_mask'], dtype='int32')\n",
    "\n",
    "        #till here and use the same shuffle on ids, masks instead of X_train\n",
    "\n",
    "        inputs = [ids, masks] \n",
    "\n",
    "        yield inputs, targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stance: Feminist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_val, y_val = reader_df(\"feminist\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **DistilBERT** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9b219e556f4fe28a616e00fc20cdd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danid\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\danid\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b04eaeb843742d8ad99a49440e43b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd94845567b543b6a03ff465d24357d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7f044bc3a647f5bd07e0705c1200e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4d2874982c4f319d9d2eee3c8fb5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/363M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\danid\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "checkpoint=\"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "batch_size=64\n",
    "max_length=64\n",
    "rate = 0.5\n",
    "num_labels = 3\n",
    "input_ids_in = tf.keras.layers.Input(shape=(max_length,), name=f'input_token', dtype='int32')\n",
    "input_masks_in = tf.keras.layers.Input(shape=(max_length,), name=f'masked_token', dtype='int32') \n",
    "\n",
    "embedding_layer = model(input_ids_in, attention_mask=input_masks_in)[0][:,0,:]\n",
    "output_layer = tf.keras.layers.Dropout(rate, name='do_layer')(embedding_layer)\n",
    "weight_initializer = tf.keras.initializers.GlorotNormal(seed=42)\n",
    "output = tf.keras.layers.Dense(num_labels, activation='softmax')(output_layer)\n",
    "bert_model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next chunk of code:\n",
    "\n",
    "\n",
    "`OneHotEncoder(handle_unknown='ignore')` creates a OneHotEncoder object with the parameter handle_unknown set to 'ignore'. This means that if the encoder encounters a new category in y_train during the encoding process, it will ignore it instead of raising an error.\n",
    "\n",
    "`y_train.reshape(-1, 1)` reshapes the 1-dimensional y_train array into a 2-dimensional array with a single column. This is required by the fit_transform() method of the OneHotEncoder object.\n",
    "\n",
    "`enc.fit_transform()` fits the encoder on the reshaped y_train data and transforms it into a one-hot encoded matrix. The resulting matrix is a sparse matrix representation of the one-hot encoded data.\n",
    "\n",
    "`.toarray()` converts the sparse matrix into a dense numpy array.\n",
    "\n",
    "`_y_train` is assigned the dense numpy array containing the one-hot encoded representation of y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 91s 9s/step - loss: 1.0744 - recall: 0.3333 - precision: 0.5175\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 76s 8s/step - loss: 0.9433 - recall: 0.3941 - precision: 0.5911\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 77s 9s/step - loss: 0.9011 - recall: 0.3958 - precision: 0.6628\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 78s 9s/step - loss: 0.7996 - recall: 0.4809 - precision: 0.6548\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 79s 9s/step - loss: 0.6826 - recall: 0.5694 - precision: 0.7523\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 81s 9s/step - loss: 0.5537 - recall: 0.7066 - precision: 0.8206\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 81s 9s/step - loss: 0.4545 - recall: 0.7812 - precision: 0.8459\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 81s 9s/step - loss: 0.3409 - recall: 0.8455 - precision: 0.8855\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 82s 9s/step - loss: 0.1984 - recall: 0.9306 - precision: 0.9487\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 82s 9s/step - loss: 0.1400 - recall: 0.9549 - precision: 0.9632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b4a9d70ac0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "_y_train = enc.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = int(len(X_train) / batch_size)\n",
    "\n",
    "batches = get_batches(X_train, _y_train, tokenizer, batch_size, max_length)\n",
    "\n",
    "bert_model.compile(optimizer=Adam(2e-5),\n",
    "                   metrics=[tf.keras.metrics.Recall(), \n",
    "                   tf.keras.metrics.Precision()],\n",
    "                   loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "\n",
    "bert_model.fit(batches, epochs=10, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 13s 1s/step\n",
      "Precision: 55.93\n",
      "Recall: 56.69\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.batch_encode_plus(list(X_test), \n",
    "                                     add_special_tokens=True, max_length=max_length, \n",
    "                                    padding='max_length',  return_attention_mask=True,\n",
    "                                    return_token_type_ids=True, truncation=True)\n",
    "\n",
    "input_test = [np.asarray(inputs['input_ids'], dtype='int32'), \n",
    "              np.asarray(inputs['attention_mask'], dtype='int32')]\n",
    "y_preds = bert_model.predict(input_test)\n",
    "y_preds = np.argmax(y_preds, axis=1)\n",
    "r_bert, p_bert = get_metrics(y_test, y_preds, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have pretty poor results. Why?\n",
    "\n",
    "BERT is a pre-trained language model that has been shown to achieve state-of-the-art performance on a wide range of natural language processing tasks. However, BERT was pre-trained on a large corpus of general text, which may not be representative of the language used in tweets.\n",
    "\n",
    "Tweets are known to have unique characteristics that can make them more challenging to classify compared to other types of text. For example, tweets are often shorter, contain a lot of noise (such as typos and slang), and can have complex grammatical structures that are not found in more formal writing.\n",
    "\n",
    "Additionally, the use of hashtags, emojis, and other special characters in tweets can make it difficult for BERT to understand the context and sentiment of the tweet. Pre-processing and cleaning the tweets can help to mitigate some of these issues, but there is still a limit to the effectiveness of this approach.\n",
    "\n",
    "To address these challenges, researchers have developed specialized versions of BERT for use with social media data. For example, BERTweet is a variant of BERT that has been trained specifically on tweets and has been shown to outperform generic BERT on tweet classification tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **BERTweet**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying BERTweet. For that we should work with the raw data, since BERTweet includes its own tokenizer method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_reader(topic):\n",
    "    path_X_train = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/stance/\" + topic + \"/train_text.txt\"\n",
    "    path_X_test = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/stance/\" + topic + \"/test_text.txt\"\n",
    "    path_X_val = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/stance/\" + topic + \"/val_text.txt\"\n",
    "    path_y_train = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/stance/\" + topic + \"/train_labels.txt\"\n",
    "    path_y_test = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/stance/\" + topic + \"/test_labels.txt\"\n",
    "    path_y_val = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/stance/\" + topic + \"/val_labels.txt\"\n",
    "\n",
    "    X_train = pd.read_table(path_X_train, header=None)\n",
    "    X_test = pd.read_table(path_X_test, header=None)\n",
    "    X_val = pd.read_table(path_X_val, header=None)\n",
    "    y_train = pd.read_table(path_y_train, header=None)\n",
    "    y_test = pd.read_table(path_y_test, header=None)\n",
    "    y_val = pd.read_table(path_y_val, header=None)\n",
    "\n",
    "    X_train = X_train.rename(columns={0: \"text\"})\n",
    "    X_test = X_test.rename(columns={0: \"text\"})\n",
    "    y_train = y_train.rename(columns={0: \"label\"})\n",
    "    y_test = y_test.rename(columns={0: \"label\"})\n",
    "    X_val = X_val.rename(columns={0: \"text\"})\n",
    "    y_val = y_val.rename(columns={0: \"label\"})\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# Load pre-trained BERTweet tokenizer and model\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to preprocess tweet data\n",
    "def preprocess_tweet(tweet):\n",
    "    # Tokenize tweet text\n",
    "    tokens = tokenizer.encode(tweet, add_special_tokens=True)\n",
    "    \n",
    "    # Truncate or pad token IDs to fixed length\n",
    "    max_len = 64\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len-1] + [tokens[-1]]\n",
    "    else:\n",
    "        tokens = tokens + [pad_token_id] * (max_len - len(tokens))\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = [1 if token != pad_token_id else 0 for token in tokens]\n",
    "    \n",
    "    return tokens, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define function to train and evaluate model\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, model, num_epochs):\n",
    "    # Define optimizer and learning rate scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "    total_steps = len(X_train) * num_epochs\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    \n",
    "    # Define loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Convert data to DataLoader objects\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.tensor([preprocess_tweet(tweet)[0] for tweet in X_train['text']]),\n",
    "                                                   torch.tensor(y_train['label'].values))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "    \n",
    "    test_dataset = torch.utils.data.TensorDataset(torch.tensor([preprocess_tweet(tweet)[0] for tweet in X_test['text']]),\n",
    "                                                  torch.tensor(y_test['label'].values))\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    test_preds = []\n",
    "    test_targets = []\n",
    "    \n",
    "    # Train model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        i=0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, attention_mask=(inputs != tokenizer.pad_token_id))\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        \n",
    "        # Evaluate model on test set\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs, attention_mask=(inputs != tokenizer.pad_token_id))\n",
    "                loss = loss_fn(outputs.logits, labels)\n",
    "                test_loss += loss.item()\n",
    "                predictions.extend\n",
    "                predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "                predictions.extend(predicted_labels.cpu().numpy())\n",
    "                targets.extend(labels.cpu().numpy())\n",
    "            test_loss /= len(test_loader)\n",
    "\n",
    "            # Store predicted and target values for computing metrics\n",
    "            test_preds.extend(predictions)\n",
    "            test_targets.extend(targets)\n",
    "        \n",
    "        # Print epoch-level metrics\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f}'.format(epoch+1, num_epochs, train_loss, test_loss))\n",
    "        \n",
    "        # Print evaluation metrics\n",
    "        precision, recall, f1_score, _ = precision_recall_fscore_support(targets, predictions, average='weighted')\n",
    "        print('Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}'.format(precision, recall, f1_score))\n",
    "\n",
    "    # Compute precision, recall, F1-score\n",
    "    test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(test_targets, test_preds, average='weighted')\n",
    "    return test_precision, test_recall, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the model on our different *stance* datasets:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Feminist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw datasets (stance feminist)\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = raw_reader(\"feminist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 0.8903, Test Loss: 0.7370\n",
      "Precision: 0.7597, Recall: 0.7692, F1 Score: 0.6972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Train Loss: 0.5966, Test Loss: 0.4844\n",
      "Precision: 0.7923, Recall: 0.8402, F1 Score: 0.8047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Train Loss: 0.3902, Test Loss: 0.4342\n",
      "Precision: 0.8013, Recall: 0.8580, F1 Score: 0.8286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Train Loss: 0.3185, Test Loss: 0.5790\n",
      "Precision: 0.7903, Recall: 0.8284, F1 Score: 0.8047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Train Loss: 0.2690, Test Loss: 0.5286\n",
      "Precision: 0.7914, Recall: 0.8402, F1 Score: 0.8140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_precision, test_recall, test_f1 = train_and_evaluate(X_train=X_train, X_test=X_test, y_test=y_test, y_train=y_train, model=model, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Stance feminist:0.7701838731802633\n",
      "Recall Stance feminist:0.8272189349112427\n",
      "F1 Stance feminist:0.7963254340425556\n"
     ]
    }
   ],
   "source": [
    "print('Precision Stance feminist:'+str(test_precision))\n",
    "print('Recall Stance feminist:'+str(test_recall))\n",
    "print('F1 Stance feminist:'+str(test_f1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw datasets (stance climate)\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = raw_reader(\"climate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 1.2727, Test Loss: 0.5541\n",
      "Precision: 0.7566, Recall: 0.8047, F1 Score: 0.7626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Train Loss: 0.3957, Test Loss: 0.4439\n",
      "Precision: 0.7891, Recall: 0.8462, F1 Score: 0.8162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danid\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Train Loss: 0.1789, Test Loss: 0.5018\n",
      "Precision: 0.8549, Recall: 0.8462, F1 Score: 0.8180\n",
      "Epoch [4/5], Train Loss: 0.0621, Test Loss: 0.7016\n",
      "Precision: 0.8306, Recall: 0.8343, F1 Score: 0.8322\n",
      "Epoch [5/5], Train Loss: 0.0765, Test Loss: 1.1228\n",
      "Precision: 0.8135, Recall: 0.7988, F1 Score: 0.8001\n"
     ]
    }
   ],
   "source": [
    "test_precision, test_recall, test_f1 = train_and_evaluate(X_train=X_train, X_test=X_test, y_test=y_test, y_train=y_train, model=model, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.8106807671546118\n",
      "Recall:0.8260355029585799\n",
      "F1:0.810752187522876\n"
     ]
    }
   ],
   "source": [
    "print('Precision Stance climate:'+str(test_precision))\n",
    "print('Recall Stance climate:'+str(test_recall))\n",
    "print('F1 Stance climate:'+str(test_f1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Abortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw datasets (stance abortion)\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = raw_reader(\"abortion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 1.0179, Test Loss: 0.9576\n",
      "Precision: 0.6602, Recall: 0.5679, F1 Score: 0.5754\n",
      "Epoch [2/5], Train Loss: 0.6347, Test Loss: 0.9178\n",
      "Precision: 0.6936, Recall: 0.5786, F1 Score: 0.5964\n",
      "Epoch [3/5], Train Loss: 0.4556, Test Loss: 0.8351\n",
      "Precision: 0.7298, Recall: 0.6214, F1 Score: 0.6394\n",
      "Epoch [4/5], Train Loss: 0.2959, Test Loss: 1.0240\n",
      "Precision: 0.7546, Recall: 0.6321, F1 Score: 0.6500\n",
      "Epoch [5/5], Train Loss: 0.1664, Test Loss: 1.1508\n",
      "Precision: 0.7496, Recall: 0.6786, F1 Score: 0.6927\n"
     ]
    }
   ],
   "source": [
    "test_precision, test_recall, test_f1 = train_and_evaluate(X_train=X_train, X_test=X_test, y_test=y_test, y_train=y_train, model=model, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Stance abortion:0.7099296754389114\n",
      "Recall Stance abortion:0.6157142857142858\n",
      "F1 Stance abortion:0.6332141118265852\n"
     ]
    }
   ],
   "source": [
    "print('Precision Stance abortion:'+str(test_precision))\n",
    "print('Recall Stance abortion:'+str(test_recall))\n",
    "print('F1 Stance abortion:'+str(test_f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Atheism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw datasets (stance abortion)\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = raw_reader(\"atheism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 0.9734, Test Loss: 0.5306\n",
      "Precision: 0.7753, Recall: 0.7909, F1 Score: 0.7724\n",
      "Epoch [2/5], Train Loss: 0.3772, Test Loss: 0.5884\n",
      "Precision: 0.8151, Recall: 0.7682, F1 Score: 0.7810\n",
      "Epoch [3/5], Train Loss: 0.2031, Test Loss: 0.8595\n",
      "Precision: 0.8117, Recall: 0.7273, F1 Score: 0.7462\n",
      "Epoch [4/5], Train Loss: 0.1186, Test Loss: 0.9208\n",
      "Precision: 0.8203, Recall: 0.7591, F1 Score: 0.7743\n",
      "Epoch [5/5], Train Loss: 0.1334, Test Loss: 0.9100\n",
      "Precision: 0.8241, Recall: 0.7455, F1 Score: 0.7650\n"
     ]
    }
   ],
   "source": [
    "test_precision, test_recall, test_f1 = train_and_evaluate(X_train=X_train, X_test=X_test, y_test=y_test, y_train=y_train, model=model, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Stance atheism:0.7976145935853368\n",
      "Recall Stance atheism:0.7581818181818182\n",
      "F1 Stance atheism:0.7706104850836318\n"
     ]
    }
   ],
   "source": [
    "print('Precision Stance atheism:'+str(test_precision))\n",
    "print('Recall Stance atheism:'+str(test_recall))\n",
    "print('F1 Stance atheism:'+str(test_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
